{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108f30ca-0c7a-4e9a-a5b8-ff293ef4851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(NOTEBOOK_DIR, '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e368ffd0-578b-474a-aeae-d75e83834bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import copy\n",
    "import collections\n",
    "\n",
    "import IPython\n",
    "from yargy.interpretation import fact as yrg_fact, attribute as yrg_attr\n",
    "from yargy.pipelines import morph_pipeline as yrg_morph_pipeline\n",
    "from yargy import rule as yrg_rule, or_ as yrg_r_or, and_ as yrg_r_and\n",
    "from yargy.predicates import \\\n",
    "    eq as yrg_rp_eq, gte as yrg_rp_gte, lte as yrg_rp_lte, type as yrg_rp_type, caseless as yrg_rp_caseless, \\\n",
    "    in_caseless as yrg_rp_in_caseless, custom as yrg_rp_custom\n",
    "from yargy import Parser as YrgParser\n",
    "from ipymarkup import show_span_ascii_markup as natasha_show_markup\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import dataset_utils\n",
    "from utils import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663c530-9dcf-4126-bbbb-b85faa2943db",
   "metadata": {},
   "source": [
    "# Search by word ontologies with Yargy parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95bd883-a975-4be6-bae8-6dbf2a182135",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61cf9941-1940-4e1c-bc84-0333f1f4677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUESTS_FILE = \"../data/request_db.txt\"\n",
    "ADS_FILE = \"../data/ads_db.txt\"\n",
    "MATCHING_FILE = \"../data/matching_db.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f51ca95-b069-496f-8806-8716bc2e30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ADS_FILE, encoding=\"utf-8\") as f:\n",
    "    ads_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56949b0-0bb4-40f3-a4df-37cee9ff1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(REQUESTS_FILE, encoding=\"utf-8\") as f:\n",
    "    requests_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "357b3da0-cecd-4312-8bb8-d78563d1e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_markup = dataset_utils.load_matching_data(MATCHING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397f814-7690-447a-819a-d64a49dc317e",
   "metadata": {},
   "source": [
    "## Constructing Ontologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d987674c-3b4e-4a8a-94a8-4f0785448288",
   "metadata": {},
   "source": [
    "### Service Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b54d07-8e27-4765-a4f6-8e23f5a9ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rule_obj_w_attrs(o_obj, n_obj, adj_dict):\n",
    "    # attributing all adjectives to the object\n",
    "    attributed_dict = {\n",
    "        # we have to copy each adjectives, because they are modified when parser is created (MorphPipelineScheme to MorphPipeline),\n",
    "        # but user can potentially pass same adjective instances for multiple objects\n",
    "        prop_name: [copy.deepcopy(adj.interpretation(getattr(o_obj, prop_name).const(adj.pipeline.lines[0]))) for adj in adj_list]\n",
    "        for prop_name, adj_list in adj_dict.items()\n",
    "    }\n",
    "\n",
    "    # generate rules for all word positions of attr adjectives and object noun\n",
    "    rule_variants = []\n",
    "    for perm_item_list in itertools.permutations(list(attributed_dict.keys()) + [n_obj]):\n",
    "        rule_variants.append(\n",
    "            yrg_rule(\n",
    "                *(\n",
    "                    yrg_r_or(*attributed_dict[p_item]).optional() if p_item is not n_obj else p_item\n",
    "                    for p_item in perm_item_list\n",
    "                )\n",
    "            ).interpretation(o_obj)\n",
    "        )\n",
    "    o_attr_variants_proxy_obj = yrg_fact(f\"{o_obj.__name__}_attr_vars_proxy\", [\"value\"])\n",
    "    high_level_or_rule = yrg_r_or(*rule_variants).interpretation(o_attr_variants_proxy_obj.value).interpretation(o_attr_variants_proxy_obj)\n",
    "\n",
    "    return high_level_or_rule\n",
    "\n",
    "\n",
    "def add_object_parser(obj_class_name, obj_noun_list, obj_prop_dict, size_rule, parser_list):\n",
    "    o_obj = yrg_fact(obj_class_name, list(obj_prop_dict.keys()))\n",
    "    n_obj = yrg_morph_pipeline(obj_noun_list)\n",
    "    r_obj = create_rule_obj_w_attrs(\n",
    "        o_obj,\n",
    "        n_obj,\n",
    "        obj_prop_dict,\n",
    "    )\n",
    "    if size_rule is not None:\n",
    "        o_obj_size_proxy = yrg_fact(f\"{obj_class_name}_size_proxy\", [\"main_obj\", \"PARSED_size_info\"])\n",
    "        r_obj = yrg_rule(\n",
    "            r_obj.interpretation(o_obj_size_proxy.main_obj),\n",
    "            yrg_rule(\n",
    "                yrg_rp_eq(\",\").optional(),\n",
    "                copy.deepcopy(size_rule),\n",
    "            ).optional().interpretation(o_obj_size_proxy.PARSED_size_info),\n",
    "        ).interpretation(o_obj_size_proxy)\n",
    "    parser_list.append(YrgParser(r_obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc16d00f-a595-43d0-a976-54bdd6e6ad1a",
   "metadata": {},
   "source": [
    "### Clothes Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71bd14f3-8438-4f1b-84d1-709cfd761c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_size_letters(token, max_x_count):\n",
    "    res = True\n",
    "    first_digits = []\n",
    "    letters_started = False\n",
    "    end_letter_reached = False\n",
    "    x_count = 0\n",
    "    for c in token:\n",
    "        if end_letter_reached:\n",
    "            res = False\n",
    "            break\n",
    "        if c.isdigit():\n",
    "            if letters_started:\n",
    "                res = False\n",
    "                break\n",
    "            first_digits.append(c)\n",
    "            continue\n",
    "        if not letters_started:\n",
    "            if len(first_digits) > 0:\n",
    "                if c.lower() != \"x\":\n",
    "                    res = False\n",
    "                    break\n",
    "                digit_val = int(\"\".join(first_digits))\n",
    "                if digit_val < 1 or digit_val > max_x_count:\n",
    "                    res = False\n",
    "                    break\n",
    "            if c.lower() not in [\"x\", \"s\", \"m\", \"l\"]:\n",
    "                res = False\n",
    "                break\n",
    "            if c.lower() in [\"s\", \"m\", \"l\"]:\n",
    "                end_letter_reached = True\n",
    "            first_digits = []\n",
    "            letters_started = True\n",
    "            continue\n",
    "        if c.lower() == \"x\":\n",
    "            x_count += 1\n",
    "            if len(first_digits) > 0 or x_count > max_x_count:\n",
    "                res = False\n",
    "                break\n",
    "            continue\n",
    "        if c.lower() not in [\"s\", \"m\", \"l\"]:\n",
    "            res = False\n",
    "            break\n",
    "        end_letter_reached = True\n",
    "    if not letters_started or not end_letter_reached:\n",
    "        res = False\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2049f09-ee2a-4984-b15f-c7b454618789",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CLOTHES_SIZE_INT = 18\n",
    "MAX_CLOTHES_SIZE_INT = 82\n",
    "MIN_CHILD_CLOTHES_SIZE_INT = MIN_CLOTHES_SIZE_INT\n",
    "MAX_CHILD_CLOTHES_SIZE_INT = 43\n",
    "MIN_W_SCHOOL_CLOTHES_SIZE_INT = 26\n",
    "MAX_W_SCHOOL_CLOTHES_SIZE_INT = 48\n",
    "MIN_M_SCHOOL_CLOTHES_SIZE_INT = 28\n",
    "MAX_M_SCHOOL_CLOTHES_SIZE_INT = 50\n",
    "MAX_CLOTHES_SIZE_X_COUNT = 12\n",
    "\n",
    "MAIN_M_GENDER_NAME_STR = \"–º—É–∂—Å–∫–æ–π\"\n",
    "MAIN_W_GENDER_NAME_STR = \"–∂–µ–Ω—Å–∫–∏–π\"\n",
    "\n",
    "rule_parsers = []\n",
    "\n",
    "# === general attributes ===\n",
    "\n",
    "gen_attributes = {}\n",
    "\n",
    "gen_attributes[\"gender\"] = [\n",
    "    yrg_morph_pipeline([\n",
    "        MAIN_M_GENDER_NAME_STR,\n",
    "        \"–º—É–∂\",\n",
    "        \"–º—É–∂.\",\n",
    "    ]),\n",
    "    yrg_morph_pipeline([\n",
    "        MAIN_W_GENDER_NAME_STR,\n",
    "        \"–∂–µ–Ω\",\n",
    "        \"–∂–µ–Ω.\",\n",
    "    ]),\n",
    "    yrg_morph_pipeline([\n",
    "        \"—É–Ω–∏—Å–µ–∫—Å\",\n",
    "        \"—é–Ω–∏—Å–µ–∫—Å\",\n",
    "    ]),\n",
    "]\n",
    "\n",
    "gen_attributes[\"season\"] = [\n",
    "    yrg_morph_pipeline([\n",
    "        \"–¥–µ–º—Å–µ–∑–æ–Ω\",\n",
    "        \"–¥–µ–º–∏—Å–µ–∑–æ–Ω\",\n",
    "        \"–¥–µ–º–∏—Å–µ–∑–æ–Ω–Ω—ã–π\",\n",
    "        \"–≤–µ—Å–µ–Ω–Ω–∏–π\",\n",
    "        \"–≤–µ—Å–Ω–∞\",\n",
    "        \"–æ—Å–µ–Ω–Ω–∏–π\",\n",
    "        \"–æ—Å–µ–Ω—å\",\n",
    "        \"–≤–µ—Å–Ω–∞-–æ—Å–µ–Ω—å\",\n",
    "        \"–æ—Å–µ–Ω—å-–≤–µ—Å–Ω–∞\",\n",
    "    ]),\n",
    "    yrg_morph_pipeline([\n",
    "        \"–∑–∏–º–Ω–∏–π\",\n",
    "        \"–∑–∏–º–∞\",\n",
    "        \"–∑–∏–º\",\n",
    "        \"–∑–∏–º–Ω\",\n",
    "    ]),\n",
    "    yrg_morph_pipeline([\n",
    "        \"–ª–µ—Ç–Ω–∏–π\",\n",
    "        \"–ª–µ—Ç–æ\",\n",
    "        \"–ª–µ—Ç\",\n",
    "        \"–ª–µ—Ç–Ω\",\n",
    "    ]),\n",
    "]\n",
    "\n",
    "gen_attributes[\"material\"] = [\n",
    "    yrg_morph_pipeline([\n",
    "        \"–¥–∂–∏–Ω—Å–æ–≤—ã–π\",\n",
    "        \"–¥–∂–∏–Ω—Å–∞\",\n",
    "    ]),\n",
    "    yrg_morph_pipeline([\n",
    "        \"–∫–æ–∂–∞–Ω—ã–π\",\n",
    "        \"–∫–æ–∂–∞\",\n",
    "    ]),\n",
    "    yrg_morph_pipeline([\n",
    "        \"—Å–∏–Ω—Ç–µ–ø–æ–Ω–æ–≤—ã–π\",\n",
    "        \"—Å–∏–Ω—Ç–µ–ø–æ–Ω\",\n",
    "    ]),\n",
    "]\n",
    "\n",
    "# === indirect size and gender information ===\n",
    "\n",
    "o_size_indirect_info = yrg_fact(\n",
    "    \"size_indirect_info\", [\"keyword\", \"year_info_from_y\", \"year_info_from_m\", \"year_info_to_y\", \"year_info_to_m\"]\n",
    ")\n",
    "r_size_gender_indirect_info = yrg_rule(\n",
    "    yrg_r_or(\n",
    "        yrg_rp_caseless(\"–Ω–∞\"),\n",
    "        yrg_rp_caseless(\"–¥–ª—è\"),\n",
    "    ).optional(),\n",
    "    yrg_morph_pipeline([\n",
    "        \"–º–∞–ª—å—á–∏–∫\",\n",
    "        \"–¥–µ–≤–æ—á–∫–∞\",\n",
    "        \"–º—É–∂—á–∏–Ω–∞\",\n",
    "        \"–∂–µ–Ω—â–∏–Ω–∞\",\n",
    "        \"—Ä–µ–±—ë–Ω–æ–∫\",\n",
    "        \"–≤–∑—Ä–æ—Å–ª—ã–π\",\n",
    "        \"—à–∫–æ–ª—å–Ω–∏–∫\",\n",
    "        \"—à–∫–æ–ª—å–Ω–∏—Ü–∞\",\n",
    "    ]).interpretation(o_size_indirect_info.keyword.normalized()),\n",
    ")\n",
    "r_size_year_info = yrg_r_or(\n",
    "    yrg_rule(\n",
    "        yrg_rp_type(\"INT\").interpretation(o_size_indirect_info.year_info_from_y),\n",
    "        yrg_rule(\n",
    "            yrg_rp_eq(\"-\"),\n",
    "            yrg_rp_type(\"INT\").interpretation(o_size_indirect_info.year_info_to_y)\n",
    "        ).optional(),\n",
    "        yrg_morph_pipeline([\"–ª–µ—Ç\", \"–≥–æ–¥\"]),\n",
    "    ),\n",
    "    yrg_rule(\n",
    "        yrg_rp_type(\"INT\").interpretation(o_size_indirect_info.year_info_from_m),\n",
    "        yrg_rule(\n",
    "            yrg_rp_eq(\"-\"),\n",
    "            yrg_rp_type(\"INT\").interpretation(o_size_indirect_info.year_info_to_m)\n",
    "        ).optional(),\n",
    "        yrg_morph_pipeline([\"–º–µ—Å—è—Ü\", \"–º–µ—Å\"]),\n",
    "    ),\n",
    ").interpretation(o_size_indirect_info)\n",
    "r_size_year_gender_indirect_info = yrg_rule(\n",
    "    r_size_gender_indirect_info,\n",
    "    r_size_year_info.optional(),\n",
    ").interpretation(o_size_indirect_info)\n",
    "\n",
    "# === direct size and gender information ===\n",
    "\n",
    "o_size_number = yrg_fact(\"size_number\", [\"int_part\", \"frac_part\"])\n",
    "r_size_number = yrg_rule(\n",
    "    yrg_r_and(\n",
    "        yrg_rp_gte(MIN_CLOTHES_SIZE_INT),\n",
    "        yrg_rp_lte(MAX_CLOTHES_SIZE_INT),\n",
    "    ).interpretation(o_size_number.int_part),\n",
    "    yrg_r_or(\n",
    "        yrg_rule(\n",
    "            yrg_rp_eq(\".\"),\n",
    "            yrg_rp_type(\"INT\").interpretation(o_size_number.frac_part),\n",
    "        ),\n",
    "        yrg_rule(\n",
    "            yrg_rp_caseless(\"—Å\"),\n",
    "            yrg_rp_caseless(\"–ø–æ–ª–æ–≤–∏–Ω–æ–π\")\n",
    "        ).interpretation(o_size_number.frac_part.const(\"5\")),\n",
    "    ).optional(),\n",
    ").interpretation(o_size_number)\n",
    "o_size_number_list = yrg_fact(\"size_number_list\", [\"from_info\", \"to_info\"])\n",
    "r_size_number_list = yrg_rule(\n",
    "    r_size_number.interpretation(o_size_number_list.from_info),\n",
    "    yrg_rule(\n",
    "        yrg_rp_eq(\"-\"),  # all types of dashes are converted to \"-\" on preprocessing\n",
    "        r_size_number.interpretation(o_size_number_list.to_info),\n",
    "    ).optional(),\n",
    ").interpretation(o_size_number_list)\n",
    "\n",
    "o_size_letters = yrg_fact(\"size_letters\", [\"letters\"])\n",
    "r_size_letters = yrg_rule(\n",
    "    yrg_r_and(   # tokenizer splits numbers from letters, so 10XL becomes '10', 'XL'\n",
    "        yrg_rp_gte(2),\n",
    "        yrg_rp_lte(MAX_CLOTHES_SIZE_X_COUNT),\n",
    "    ).optional(),\n",
    "    yrg_rp_custom(lambda tok: is_size_letters(tok, MAX_CLOTHES_SIZE_X_COUNT)),\n",
    ").interpretation(o_size_letters.letters).interpretation(o_size_letters)\n",
    "o_size_letters_list = yrg_fact(\"size_letters_list\", [\"from_info\", \"to_info\"])\n",
    "r_size_letters_list = yrg_rule(\n",
    "    r_size_letters.interpretation(o_size_letters_list.from_info),\n",
    "    yrg_rule(\n",
    "        yrg_rp_eq(\"-\"),  # all types of dashes are converted to \"-\" on preprocessing\n",
    "        r_size_letters.interpretation(o_size_letters_list.to_info),\n",
    "    ).optional(),\n",
    ").interpretation(o_size_letters_list)\n",
    "\n",
    "n_size_word = yrg_morph_pipeline([\n",
    "    \"—Ä–∞–∑–º–µ—Ä\",\n",
    "    \"—Ä\",\n",
    "    \"p.\",\n",
    "])\n",
    "o_size_direct_values = yrg_fact(\"size_direct_values\", [\"direct_values\"])\n",
    "r_size_direct_values = yrg_r_or(\n",
    "    yrg_rule(\n",
    "        n_size_word.optional(),\n",
    "        yrg_r_or(\n",
    "            r_size_number_list,\n",
    "            r_size_letters_list,\n",
    "        ).interpretation(o_size_direct_values.direct_values),\n",
    "    ),\n",
    "    yrg_rule(\n",
    "        r_size_number_list,\n",
    "        n_size_word,\n",
    "    ).interpretation(o_size_direct_values.direct_values),\n",
    ").interpretation(o_size_direct_values)\n",
    "\n",
    "# === general size information ===\n",
    "\n",
    "o_size_info = yrg_fact(\"size_info\", [\"direct_values\", \"indirect_values\"])\n",
    "r_size_info = yrg_r_or(\n",
    "    r_size_year_gender_indirect_info.interpretation(o_size_info.indirect_values),\n",
    "    r_size_direct_values.interpretation(o_size_info.direct_values),\n",
    ").interpretation(o_size_info)\n",
    "\n",
    "# === objects ===\n",
    "\n",
    "add_object_parser(\n",
    "    obj_class_name=\"Coat\",\n",
    "    obj_noun_list=[\n",
    "        \"–ø–∞–ª—å—Ç–æ\",\n",
    "        \"–ø–æ–ª—É–ø–∞–ª—å—Ç–æ\",\n",
    "    ],\n",
    "    obj_prop_dict={\n",
    "        **gen_attributes,\n",
    "    },\n",
    "    size_rule=r_size_info,\n",
    "    parser_list=rule_parsers,\n",
    ")\n",
    "\n",
    "add_object_parser(\n",
    "    obj_class_name=\"Jacket\",\n",
    "    obj_noun_list=[\n",
    "        \"–∫—É—Ä—Ç–∫–∞\",\n",
    "        \"–≤–µ—Ç—Ä–æ–≤–∫–∞\",\n",
    "        \"–±–æ–º–±–µ—Ä\",\n",
    "        \"–∫—É—Ä—Ç–∫–∞-–±–æ–º–±–µ—Ä\",\n",
    "        \"–ª–µ—Ç–Ω–∞—è –∫—É—Ä—Ç–∫–∞\",\n",
    "        \"–∫—É—Ä—Ç–∫–∞ –ª–µ—Ç–Ω–∞—è\",\n",
    "    ],\n",
    "    obj_prop_dict={\n",
    "        **gen_attributes,\n",
    "    },\n",
    "    size_rule=r_size_info,\n",
    "    parser_list=rule_parsers,\n",
    ")\n",
    "\n",
    "add_object_parser(\n",
    "    obj_class_name=\"Sweater\",\n",
    "    obj_noun_list=[\n",
    "        \"–∫–æ—Ñ—Ç–∞\",\n",
    "        \"—Å–≤–∏—Ç–µ—Ä\",\n",
    "    ],\n",
    "    obj_prop_dict={\n",
    "        **gen_attributes,\n",
    "    },\n",
    "    size_rule=r_size_info,\n",
    "    parser_list=rule_parsers,\n",
    ")\n",
    "\n",
    "add_object_parser(\n",
    "    obj_class_name=\"Blouse\",\n",
    "    obj_noun_list=[\n",
    "        \"–±–ª—É–∑–∫–∞\",\n",
    "    ],\n",
    "    obj_prop_dict={\n",
    "        **{k: v for k, v in gen_attributes.items() if k != \"gender\"},  # it is supposed that blouses are only for women\n",
    "    },\n",
    "    size_rule=r_size_info,\n",
    "    parser_list=rule_parsers,\n",
    ")\n",
    "\n",
    "add_object_parser(\n",
    "    obj_class_name=\"Trousers\",\n",
    "    obj_noun_list=[\n",
    "        \"—à—Ç–∞–Ω—ã\",\n",
    "        \"–¥–∂–∏–Ω—Å—ã\",\n",
    "    ],\n",
    "    obj_prop_dict={\n",
    "        **gen_attributes,\n",
    "    },\n",
    "    size_rule=r_size_info,\n",
    "    parser_list=rule_parsers,\n",
    ")\n",
    "\n",
    "add_object_parser(\n",
    "    obj_class_name=\"Skirt\",\n",
    "    obj_noun_list=[\n",
    "        \"—é–±–∫–∞\",\n",
    "    ],\n",
    "    obj_prop_dict={\n",
    "        **{k: v for k, v in gen_attributes.items() if k != \"gender\"},  # it is supposed that skirts are only for women\n",
    "    },\n",
    "    size_rule=r_size_info,\n",
    "    parser_list=rule_parsers,\n",
    ")\n",
    "\n",
    "add_object_parser(\n",
    "    obj_class_name=\"Shirt\",\n",
    "    obj_noun_list=[\n",
    "        \"—Ä—É–±–∞—à–∫–∞\",\n",
    "    ],\n",
    "    obj_prop_dict={\n",
    "        **gen_attributes,\n",
    "    },\n",
    "    size_rule=r_size_info,\n",
    "    parser_list=rule_parsers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a632bf8f-4f10-41de-9555-9e865de8722c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match(tokens=[MorphToken(value='–Ω–∞', span=[7, 9), type='RU', forms=[Form('–Ω–∞', Grams(PREP)), Form('–Ω–∞', Grams(PRCL)), Form('–Ω–∞', Grams(INTJ))]), MorphToken(value='–º–∞–ª—å—á–∏–∫–∞', span=[10, 18), type='RU', forms=[Form('–º–∞–ª—å—á–∏–∫', Grams(NOUN,accs,anim,masc,sing)), Form('–º–∞–ª—å—á–∏–∫', Grams(NOUN,anim,gent,masc,sing))]), Token(value='4', span=[19, 20), type='INT'), Token(value='-', span=[20, 21), type='PUNCT'), Token(value='6', span=[21, 22), type='INT'), MorphToken(value='–ª–µ—Ç', span=[23, 26), type='RU', forms=[Form('–≥–æ–¥', Grams(NOUN,gent,inan,masc,plur)), Form('–ª—ë—Ç', Grams(NOUN,inan,masc,nomn,sing)), Form('–ª—ë—Ç', Grams(NOUN,accs,inan,masc,sing))])], span=[7, 26))\n"
     ]
    }
   ],
   "source": [
    "parser = YrgParser(r_size_year_gender_indirect_info)\n",
    "matches = parser.findall(\"–æ–¥–µ–∂–¥–∞ –Ω–∞ –º–∞–ª—å—á–∏–∫–∞ 4-6 –ª–µ—Ç\")\n",
    "for m in matches:\n",
    "    print(m)\n",
    "    # print(f\"{m.tree.root.production}\")\n",
    "    # print(f\"{m.tree.root.production.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "421f5c44-54a8-4a82-b14a-354f39c9574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match(tokens=[MorphToken(value='—Å–≤–∏—Ç–µ—Ä', span=[0, 6), type='RU', forms=[Form('—Å–≤–∏—Ç–µ—Ä', Grams(NOUN,inan,masc,nomn,sing)), Form('—Å–≤–∏—Ç–µ—Ä', Grams(NOUN,accs,inan,masc,sing))]), MorphToken(value='–Ω–∞', span=[7, 9), type='RU', forms=[Form('–Ω–∞', Grams(PREP)), Form('–Ω–∞', Grams(PRCL)), Form('–Ω–∞', Grams(INTJ))]), MorphToken(value='–º–∞–ª—å—á–∏–∫–∞', span=[10, 18), type='RU', forms=[Form('–º–∞–ª—å—á–∏–∫', Grams(NOUN,accs,anim,masc,sing)), Form('–º–∞–ª—å—á–∏–∫', Grams(NOUN,anim,gent,masc,sing))]), Token(value='4', span=[19, 20), type='INT'), Token(value='-', span=[20, 21), type='PUNCT'), Token(value='6', span=[21, 22), type='INT'), MorphToken(value='–ª–µ—Ç', span=[23, 26), type='RU', forms=[Form('–≥–æ–¥', Grams(NOUN,gent,inan,masc,plur)), Form('–ª—ë—Ç', Grams(NOUN,inan,masc,nomn,sing)), Form('–ª—ë—Ç', Grams(NOUN,accs,inan,masc,sing))])], span=[0, 26))\n"
     ]
    }
   ],
   "source": [
    "matches = rule_parsers[2].findall(\"—Å–≤–∏—Ç–µ—Ä –Ω–∞ –º–∞–ª—å—á–∏–∫–∞ 4-6 –ª–µ—Ç\")\n",
    "for m in matches:\n",
    "    print(m)\n",
    "    # print(f\"{m.tree.root.production}\")\n",
    "    # print(f\"{m.tree.root.production.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6987510-e6d2-4d41-be00-3813b22e58cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match(tokens=[MorphToken(value='—Å–≤–∏—Ç–µ—Ä', span=[0, 6), type='RU', forms=[Form('—Å–≤–∏—Ç–µ—Ä', Grams(NOUN,inan,masc,nomn,sing)), Form('—Å–≤–∏—Ç–µ—Ä', Grams(NOUN,accs,inan,masc,sing))]), MorphToken(value='–Ω–∞', span=[7, 9), type='RU', forms=[Form('–Ω–∞', Grams(PREP)), Form('–Ω–∞', Grams(PRCL)), Form('–Ω–∞', Grams(INTJ))]), MorphToken(value='—à–∫–æ–ª—å–Ω–∏—Ü—É', span=[10, 19), type='RU', forms=[Form('—à–∫–æ–ª—å–Ω–∏—Ü–∞', Grams(NOUN,accs,anim,femn,sing))])], span=[0, 19))\n"
     ]
    }
   ],
   "source": [
    "matches = rule_parsers[2].findall(\"—Å–≤–∏—Ç–µ—Ä –Ω–∞ —à–∫–æ–ª—å–Ω–∏—Ü—É 46-48 —Ä–∞–∑–º–µ—Ä–∞\")\n",
    "for m in matches:\n",
    "    print(m)\n",
    "    # print(f\"{m.tree.root.production}\")\n",
    "    # print(f\"{m.tree.root.production.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e08fd270-a067-453b-9503-789508c6af81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match(tokens=[MorphToken(value='–ü–æ–ª—É–ø–∞–ª—å—Ç–æ', span=[38, 48), type='RU', forms=[Form('–ø–æ–ª—É–ø–∞–ª—å—Ç–æ', Grams(Fixd,NOUN,inan,neut,nomn,sing)), Form('–ø–æ–ª—É–ø–∞–ª—å—Ç–æ', Grams(Fixd,NOUN,gent,inan,neut,sing)), Form('–ø–æ–ª—É–ø–∞–ª—å—Ç–æ', Grams(Fixd,NOUN,datv,inan,neut,sing)), Form('–ø–æ–ª—É–ø–∞–ª—å—Ç–æ', Grams(Fixd,NOUN,accs,inan,neut,sing)), Form('–ø–æ–ª—É–ø–∞–ª—å—Ç–æ', Grams(Fixd,NOUN,ablt,inan,neut,sing)), Form('–ø–æ–ª—É–ø–∞–ª—å—Ç–æ', Grams(Fixd,NOUN,inan,loct,neut,sing)), Form('–ø–æ–ª—É–ø–∞–ª—å—Ç–æ', Grams(Fixd,NOUN,inan,neut,nomn,plur)), Form('–ø–æ–ª—É–ø–∞–ª—å—Ç–æ', Grams(Fixd,NOUN,gent,inan,neut,plur)), Form('–ø–æ–ª—É–ø–∞–ª—å—Ç–æ', Grams(Fixd,NOUN,datv,inan,neut,plur)), Form('–ø–æ–ª—É–ø–∞–ª—å—Ç–æ', Grams(Fixd,NOUN,accs,inan,neut,plur)), Form('–ø–æ–ª—É–ø–∞–ª—å—Ç–æ', Grams(Fixd,NOUN,ablt,inan,neut,plur)), Form('–ø–æ–ª—É–ø–∞–ª—å—Ç–æ', Grams(Fixd,NOUN,inan,loct,neut,plur))])], span=[38, 48))\n"
     ]
    }
   ],
   "source": [
    "matches = rule_parsers[0].findall(\"–û—Ç–¥–∞–º –±–µ—Å–ø–ª–∞—Ç–Ω–æ –ø–∞–∫–µ—Ç–æ–º –≤ –æ–¥–Ω–∏ —Ä—É–∫–∏.\\\\n–ü–æ–ª—É–ø–∞–ª—å—Ç–æ, –ø–µ—Ä—á–∞—Ç–∫–∏, —Å—É–º–∫–∞ –∏ –≤–µ—â–∏ 42-–≥–æ —Ä–∞–∑–º–µ—Ä–∞.\")\n",
    "for m in matches:\n",
    "    print(m)\n",
    "    # print(f\"{m.tree.root.production}\")\n",
    "    # print(f\"{m.tree.root.production.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b6c62e3-409e-49a1-81a2-3b51da82a715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rules for 7 objects were created\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rules for {len(rule_parsers)} objects were created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28727212-7adb-4ff6-b021-1b7aeaa74c3f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a2165f3-3f5e-406f-90da-ecbc1e852cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert \"—ë\" to \"–µ\", correct typos, correct terms, correct (unify) dashes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "534de60e-4bb7-4431-9897-9c72bd524579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_letter_toks_to_value(size_letters, gender_name, max_x_count):\n",
    "\n",
    "    def lead_number_to_x(size_info, max_x_count):\n",
    "        first_digits = []\n",
    "        letters_started = False\n",
    "        end_letter_reached = False\n",
    "        res = []\n",
    "        for pos, c in enumerate(size_info):\n",
    "            if c.isdigit():\n",
    "                first_digits.append(c)\n",
    "                continue\n",
    "            if len(first_digits) > 0:\n",
    "                digit_val = max(1, min(int(\"\".join(first_digits)), max_x_count))\n",
    "                res = \"\".join([\"x\"] * digit_val)\n",
    "                if c.lower() != \"x\":\n",
    "                    res += size_info[pos:]\n",
    "                else:\n",
    "                    res += size_info[pos + 1:]\n",
    "            else:\n",
    "                res = size_info\n",
    "            break\n",
    "        return res.lower()\n",
    "\n",
    "    def letters_to_range(letters, gender_code):\n",
    "        m_letters_to_size_map = {\n",
    "            'xs': (40, 44),\n",
    "            's': (42, 48),\n",
    "            'm': (44, 50),\n",
    "            'l': (48, 52),\n",
    "            'xl': (50, 56),\n",
    "            'xxl': (52, 60),\n",
    "            'xxxl': (54, 64),\n",
    "            'xxxxl': (56, 66),\n",
    "            'xxxxxl': (58, 70),\n",
    "            'xxxxxxl': (60, 72),\n",
    "            'xxxxxxxl': (62, 74),\n",
    "            'xxxxxxxxl': (64, 76),\n",
    "            'xxxxxxxxxl': (66, 78),\n",
    "            'xxxxxxxxxxl': (68, 80),\n",
    "        }\n",
    "        w_letters_to_size_map = {\n",
    "            'xxxs': (36, 36),\n",
    "            'xxs': (38, 38),\n",
    "            'xs': (38, 44),\n",
    "            's': (42, 46),\n",
    "            'm': (44, 48),\n",
    "            'l': (46, 50),\n",
    "            'xl': (48, 54),\n",
    "            'xxl': (50, 58),\n",
    "            'xxxl': (52, 64),\n",
    "            'xxxxl': (54, 66),\n",
    "            'xxxxxl': (56, 70),\n",
    "            'xxxxxxl': (58, 74),\n",
    "            'xxxxxxxl': (56, 78),\n",
    "            'xxxxxxxxl': (58, 82),\n",
    "        }\n",
    "\n",
    "        if gender_code == 'm':\n",
    "            mapper = m_letters_to_size_map\n",
    "        else:\n",
    "            mapper = w_letters_to_size_map\n",
    "\n",
    "        if letters not in mapper:\n",
    "            if letters[-1] == \"l\":\n",
    "                res_range = (max(max(v) for v in mapper.values()), MAX_CLOTHES_SIZE_INT)\n",
    "            else:\n",
    "                res_range = (MIN_CLOTHES_SIZE_INT, min(min(v) for v in mapper.values()))\n",
    "        else:\n",
    "            res_range = mapper[letters]\n",
    "\n",
    "        return res_range\n",
    "\n",
    "    size_letters = lead_number_to_x(size_letters, max_x_count)\n",
    "\n",
    "    if gender_name is None:\n",
    "        m_range = letters_to_range(size_letters, 'm')\n",
    "        w_range = letters_to_range(size_letters, 'w')\n",
    "        size_range = (min(m_range[0], w_range[0]), max(m_range[1], w_range[1]))\n",
    "    elif gender_name == MAIN_M_GENDER_NAME_STR:\n",
    "        size_range = letters_to_range(size_letters, 'm')\n",
    "    elif gender_name == MAIN_W_GENDER_NAME_STR:\n",
    "        size_range = letters_to_range(size_letters, 'w')\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown gender name: {gender_name}\")\n",
    "\n",
    "    return size_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e08c9a7-8b26-43a6-877c-4f78ea02bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_size_info(orig_fact):\n",
    "\n",
    "    def direct_info_to_range(fact, gender_name):\n",
    "\n",
    "        def _number_toks_to_value(number_info):\n",
    "            if number_info.frac_part is not None:\n",
    "                res = float(f\"{number_info.int_part}.{number_info.frac_part}\")\n",
    "            else:\n",
    "                res = int(number_info.int_part)\n",
    "            return res\n",
    "\n",
    "        size_info = fact.direct_values\n",
    "        info_type = size_info.__class__.__name__\n",
    "        if info_type == \"size_number_list\":\n",
    "            size_from = _number_toks_to_value(size_info.from_info)\n",
    "            if size_info.to_info is None:\n",
    "                size_to = size_from\n",
    "            else:\n",
    "                size_to = _number_toks_to_value(size_info.to_info)\n",
    "            size_range = (size_from, size_to)\n",
    "        elif info_type == \"size_letters_list\":\n",
    "            range_from = size_letter_toks_to_value(size_info.from_info.letters, gender_name, MAX_CLOTHES_SIZE_X_COUNT)\n",
    "            if size_info.to_info is None:\n",
    "                range_to = range_from\n",
    "            else:\n",
    "                range_to = size_letter_toks_to_value(size_info.to_info.letters, gender_name, MAX_CLOTHES_SIZE_X_COUNT)\n",
    "            size_range = (min(range_from), max(range_to))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown info type \\\"{info_type}\\\"\")\n",
    "\n",
    "        return size_range\n",
    "\n",
    "    def indirect_info_to_range(fact, main_obj):\n",
    "        size_info = fact\n",
    "        if size_info.keyword == \"–º–∞–ª—å—á–∏–∫\":\n",
    "            if hasattr(main_obj.value, \"gender\"):\n",
    "                main_obj.value.gender = MAIN_M_GENDER_NAME_STR\n",
    "            size_range = (MIN_CHILD_CLOTHES_SIZE_INT, MAX_CHILD_CLOTHES_SIZE_INT)\n",
    "        elif size_info.keyword == \"–¥–µ–≤–æ—á–∫–∞\":\n",
    "            if hasattr(main_obj.value, \"gender\"):\n",
    "                main_obj.value.gender = MAIN_W_GENDER_NAME_STR\n",
    "            size_range = (MIN_CHILD_CLOTHES_SIZE_INT, MAX_CHILD_CLOTHES_SIZE_INT)\n",
    "        elif size_info.keyword == \"–º—É–∂—á–∏–Ω–∞\":\n",
    "            if hasattr(main_obj.value, \"gender\"):\n",
    "                main_obj.value.gender = MAIN_M_GENDER_NAME_STR\n",
    "            size_range = (MAX_CHILD_CLOTHES_SIZE_INT, MAX_CLOTHES_SIZE_INT)\n",
    "        elif size_info.keyword == \"–∂–µ–Ω—â–∏–Ω–∞\":\n",
    "            if hasattr(main_obj.value, \"gender\"):\n",
    "                main_obj.value.gender = MAIN_W_GENDER_NAME_STR\n",
    "            size_range = (MAX_CHILD_CLOTHES_SIZE_INT, MAX_CLOTHES_SIZE_INT)\n",
    "        elif size_info.keyword == \"—Ä–µ–±—ë–Ω–æ–∫\":\n",
    "            size_range = (MIN_CLOTHES_SIZE_INT, MAX_CHILD_CLOTHES_SIZE_INT)\n",
    "        elif size_info.keyword == \"–≤–∑—Ä–æ—Å–ª—ã–π\":\n",
    "            size_range = (MAX_CHILD_CLOTHES_SIZE_INT, MAX_CLOTHES_SIZE_INT)\n",
    "        elif size_info.keyword == \"—à–∫–æ–ª—å–Ω–∏–∫\":\n",
    "            # in some cases this word can also be applicable to women\n",
    "            if hasattr(main_obj.value, \"gender\") and fact.main_obj.value.gender is None:\n",
    "                main_obj.value.gender = MAIN_M_GENDER_NAME_STR\n",
    "            size_range = (MIN_M_SCHOOL_CLOTHES_SIZE_INT, MAX_M_SCHOOL_CLOTHES_SIZE_INT)\n",
    "        elif size_info.keyword == \"—à–∫–æ–ª—å–Ω–∏—Ü–∞\":\n",
    "            if hasattr(main_obj.value, \"gender\"):\n",
    "                main_obj.value.gender = MAIN_W_GENDER_NAME_STR\n",
    "            size_range = (MIN_W_SCHOOL_CLOTHES_SIZE_INT, MAX_W_SCHOOL_CLOTHES_SIZE_INT)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown keyword: {fact.size_info.keyword}\")\n",
    "\n",
    "        if size_info.year_info_from_y is not None:\n",
    "            year_to_size_map = {\n",
    "                0: (18, 26),\n",
    "                1: (26, 28),\n",
    "                2: (28, 30),\n",
    "                3: (28, 30),\n",
    "                4: (30, 30),\n",
    "                5: (30, 32),\n",
    "                6: (32, 34),\n",
    "                7: (34, 36),\n",
    "                8: (34, 36),\n",
    "                9: (36, 36),\n",
    "                10: (36, 36),\n",
    "                11: (36, 38),\n",
    "                12: (36, 38),\n",
    "                13: (38, 40),\n",
    "                14: (38, 40),\n",
    "            }\n",
    "            if size_info.year_info_to_y is None:\n",
    "                size_info.year_info_to_y = size_info.year_info_from_y\n",
    "            from_y = int(size_info.year_info_from_y)\n",
    "            to_y = int(size_info.year_info_to_y)\n",
    "\n",
    "            size_from = year_to_size_map.get(from_y, (MAX_CHILD_CLOTHES_SIZE_INT, size_range[1]))\n",
    "            size_to = year_to_size_map.get(to_y, (size_range[0], MAX_CLOTHES_SIZE_INT))\n",
    "            size_range = (min(size_from), max(size_to))\n",
    "        elif size_info.year_info_from_m is not None:\n",
    "            month_to_size_map = {\n",
    "                0: (18, 18),\n",
    "                1: (18, 20),\n",
    "                2: (18, 20),\n",
    "                3: (18, 22),\n",
    "                4: (20, 22),\n",
    "                5: (20, 22),\n",
    "                6: (20, 24),\n",
    "                7: (22, 24),\n",
    "                8: (22, 24),\n",
    "                9: (22, 26),\n",
    "                10: (24, 26),\n",
    "                11: (24, 26),\n",
    "                12: (24, 26),\n",
    "            }\n",
    "            if size_info.year_info_to_m is None:\n",
    "                size_info.year_info_to_m = size_info.year_info_from_m\n",
    "            from_m = int(size_info.year_info_from_m)\n",
    "            to_m = int(size_info.year_info_to_m)\n",
    "\n",
    "            size_from = month_to_size_map.get(from_m, (MAX_CHILD_CLOTHES_SIZE_INT, size_range[1]))\n",
    "            size_to = month_to_size_map.get(to_m, (size_range[0], MAX_CLOTHES_SIZE_INT))\n",
    "            size_range = (min(size_from), max(size_to))\n",
    "        else:\n",
    "            # no info is present\n",
    "            pass\n",
    "\n",
    "        return size_range\n",
    "\n",
    "    if orig_fact.PARSED_size_info is None:\n",
    "        return orig_fact\n",
    "   \n",
    "    obj_class_name = orig_fact.PARSED_size_info.__class__.__name__\n",
    "    if obj_class_name == \"size_info\":\n",
    "        if orig_fact.PARSED_size_info.direct_values is not None:\n",
    "            size_range = direct_info_to_range(orig_fact.PARSED_size_info.direct_values, orig_fact.main_obj.value.gender)\n",
    "        elif orig_fact.PARSED_size_info.indirect_values is not None:\n",
    "            size_range = indirect_info_to_range(orig_fact.PARSED_size_info.indirect_values, orig_fact.main_obj)\n",
    "        else:\n",
    "            raise ValueError(\"Both size infos are None, while object itself is not\")\n",
    "    else:\n",
    "        raise ValueError(f\"No handler for object \\\"{obj_class_name}\\\"\")\n",
    "\n",
    "    if size_range[0] > size_range[1]:\n",
    "        size_range = (size_range[1], size_range[0])\n",
    "\n",
    "    orig_fact.PARSED_size_info = size_range\n",
    "    assert isinstance(orig_fact.PARSED_size_info, tuple) and len(orig_fact.PARSED_size_info) == 2\n",
    "\n",
    "    return orig_fact\n",
    "\n",
    "\n",
    "def get_facts(text, rule_parsers):\n",
    "    trees = []\n",
    "    for parser in rule_parsers:\n",
    "        matched_trees = list(parser.findall(text))\n",
    "        if len(matched_trees) == 0:\n",
    "            continue\n",
    "        # for each parser we take only longest matches, that aren't overlapped from left to right\n",
    "        matched_trees = sorted(matched_trees, key=lambda m: (m.span.stop - m.span.start, m.span.start), reverse=True)\n",
    "        taken_trees = [matched_trees[0]]\n",
    "        for m_tree in matched_trees[1:]:\n",
    "            if all(m_tree.span.stop <= taken_tree.span.start or m_tree.span.start >= taken_tree.span.stop for taken_tree in taken_trees):\n",
    "                taken_trees.append(m_tree)\n",
    "        trees += taken_trees\n",
    "    return [decode_size_info(tree.fact) for tree in trees]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aad495b-ad4e-4a7e-881f-c6c6047e0e9e",
   "metadata": {},
   "source": [
    "Words are conversted to normal form by parsers, so text preprocessing is not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cd68dc9-718f-498f-8263-1630a72cd078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Coat_size_proxy(\n",
       "     main_obj=Coat_attr_vars_proxy(\n",
       "         value=Coat(\n",
       "             gender=None,\n",
       "             season=None,\n",
       "             material=None\n",
       "         )\n",
       "     ),\n",
       "     PARSED_size_info=(44,\n",
       "      82)\n",
       " )]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_facts(\"–ø–∞–ª—å—Ç–æ —Ä–∞–∑–º–µ—Ä–∞ M - 12xXL\", rule_parsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "705567dc-e034-4907-876c-b0479fb53750",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ad_facts = [get_facts(text, rule_parsers) for text in ads_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ed4aa64-72c6-498a-bacb-5543e799d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_req_facts = [get_facts(text, rule_parsers) for text in requests_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "737805a6-17d7-4df6-8348-7dd715af3bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coat_size_proxy: 4 advertisements, 33 requests\n",
      "Sweater_size_proxy: 4 advertisements, 0 requests\n",
      "Trousers_size_proxy: 5 advertisements, 1 requests\n",
      "Blouse_size_proxy: 1 advertisements, 0 requests\n",
      "Shirt_size_proxy: 2 advertisements, 1 requests\n",
      "Jacket_size_proxy: 3 advertisements, 24 requests\n",
      "Skirt_size_proxy: 4 advertisements, 0 requests\n"
     ]
    }
   ],
   "source": [
    "fact_counts = {}\n",
    "for ad_facts in all_ad_facts:\n",
    "    for ad_fact in ad_facts:\n",
    "        f_name = ad_fact.__class__.__name__\n",
    "        if f_name not in fact_counts:\n",
    "            fact_counts[f_name] = [0, 0]\n",
    "        fact_counts[f_name][0] += 1\n",
    "for req_facts in all_req_facts:\n",
    "    for req_fact in req_facts:\n",
    "        f_name = req_fact.__class__.__name__\n",
    "        if f_name not in fact_counts:\n",
    "            fact_counts[f_name] = [0, 0]\n",
    "        fact_counts[f_name][1] += 1\n",
    "\n",
    "for fact_name, (ad_cnt, req_cnt) in fact_counts.items():\n",
    "    print(f\"{fact_name}: {ad_cnt} advertisements, {req_cnt} requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a0e7106-9d52-4007-b6e1-8688b4412bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.8 ms, sys: 0 ns, total: 26.8 ms\n",
      "Wall time: 26.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Jacket_size_proxy(\n",
       "     main_obj=Jacket_attr_vars_proxy(\n",
       "         value=Jacket(\n",
       "             gender=None,\n",
       "             season=None,\n",
       "             material='–¥–∂–∏–Ω—Å–æ–≤—ã–π'\n",
       "         )\n",
       "     ),\n",
       "     PARSED_size_info=None\n",
       " ),\n",
       " Sweater_size_proxy(\n",
       "     main_obj=Sweater_attr_vars_proxy(\n",
       "         value=Sweater(\n",
       "             gender=None,\n",
       "             season=None,\n",
       "             material=None\n",
       "         )\n",
       "     ),\n",
       "     PARSED_size_info=None\n",
       " )]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_facts(\"–¥–∂–∏–Ω—Å–æ–≤—ã–µ –∫—É—Ä—Ç–∫–∞ —Å –∫–æ—Ñ—Ç–æ–π\", rule_parsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b9b4d19-b594-42c4-9b1c-57641578b99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.2 ms, sys: 1 Œºs, total: 26.2 ms\n",
      "Wall time: 25.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Jacket_size_proxy(\n",
       "     main_obj=Jacket_attr_vars_proxy(\n",
       "         value=Jacket(\n",
       "             gender=None,\n",
       "             season=None,\n",
       "             material=None\n",
       "         )\n",
       "     ),\n",
       "     PARSED_size_info=None\n",
       " )]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_facts(\"–∫—É—Ä—Ç–∫–∞ –∏–∑ –∫–æ–∂–∏\", rule_parsers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcec9e2-fa95-49e3-94c1-ac70bf0fb979",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0701cd74-2739-454c-b72a-b2c42281ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_facts_close(req_facts, ad_facts):\n",
    "    for req_fact in req_facts:\n",
    "        for ad_fact in ad_facts:\n",
    "            if req_fact.__class__.__name__ != ad_fact.__class__.__name__:\n",
    "                continue\n",
    "            is_match = True\n",
    "            for attr_name in req_fact.__attributes__:\n",
    "                ad_attr = getattr(ad_fact, attr_name)\n",
    "                req_attr = getattr(req_fact, attr_name)\n",
    "                if req_attr is not None and ad_attr is not None:\n",
    "                    # different attributes are not match, but if this attribute is omitted in request or ad, this is still match\n",
    "                    if attr_name == \"PARSED_size_info\":\n",
    "                        if max(req_attr) < min(ad_attr) or min(req_attr) > max(ad_attr):\n",
    "                            # any intersection of sized is a match, but no intersection means no metch\n",
    "                            is_match = False\n",
    "                            break\n",
    "                    elif req_attr != ad_attr:\n",
    "                        is_match = False\n",
    "                        break\n",
    "            if not is_match:\n",
    "                continue\n",
    "            # even one matched fact is complete match between request and ad\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def predict_by_facts(req_fact_list, ad_fact_list):\n",
    "    predictions = {}\n",
    "    for req_id, req_facts in enumerate(req_fact_list, start=1):\n",
    "        found_list = []\n",
    "        for ad_id, ad_facts in enumerate(ad_fact_list, start=1):\n",
    "            if are_facts_close(req_facts, ad_facts):\n",
    "                found_list.append(str(ad_id))\n",
    "        if len(found_list) > 0:\n",
    "            predictions[str(req_id)] = found_list.copy()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "665f8125-9152-4406-9c25-884c7b619632",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_markup = predict_by_facts(all_req_facts, all_ad_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70d88008-02ea-4689-98c6-b2167c8450bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 54, 'FP': 23, 'TN': 87202, 'FN': 543}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = metrics.calc_confusion_matrix(true_markup, pred_markup, n_ads=len(ads_raw), n_requests=len(requests_raw))\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3aaa536-a1ee-41dd-8fa8-7d30d1f44af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.99355514563549,\n",
       " 'precision': 0.7012987012987013,\n",
       " 'recall': 0.09045226130653267,\n",
       " 'f1': 0.16023738872403562}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = metrics.calc_all_stats(confusion_matrix)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83872ea7-2336-44db-a648-ab10741196fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "|\tMetric\t\t|\tOld Value\t|\tNew Value\t|\tDiff\t|\n",
      "-----------------------------------------------------------------------------------------\n",
      "|\tTP\t\t|\t216\t\t|\t54\t\t|\tüìâ -162\t|\n",
      "|\tFP\t\t|\t418\t\t|\t23\t\t|\tüìâ -395\t|\n",
      "|\tTN\t\t|\t86810\t\t|\t87202\t\t|\tüìà 392\t|\n",
      "|\tFN\t\t|\t378\t\t|\t543\t\t|\tüìà 165\t|\n",
      "|\tPrec\t\t|\t0.341\t\t|\t0.701\t\t|\tüìà 0.361\t|\n",
      "|\tRecall\t\t|\t0.364\t\t|\t0.090\t\t|\tüìâ -0.273\t|\n",
      "|\tF1\t\t|\t0.352\t\t|\t0.160\t\t|\tüìâ -0.192\t|\n",
      "\n",
      "F1 üìâ decreased by 0.192, down to 16.0%, which is a significant fall.\n"
     ]
    }
   ],
   "source": [
    "metrics.compare_with_saved_stats(stats, confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a86a0c-8e79-4235-a4de-817e8fdf9eb4",
   "metadata": {},
   "source": [
    "## Topics for Learning Yargy\n",
    "\n",
    "Documentation:\n",
    "* https://nbviewer.org/github/natasha/yargy/blob/master/docs/index.ipynb\n",
    "* https://nbviewer.org/github/natasha/yargy/blob/master/docs/ref.ipynb\n",
    "* https://nbviewer.org/github/natasha/yargy/blob/master/docs/cookbook.ipynb\n",
    "\n",
    "Topics for paying attention to:\n",
    "1. Main terms and entities: rule, fact (+interpretation stage), predicate, gazetteer\n",
    "1. Multiple values for single attribute are not supported\n",
    "1. Rules for arbitrary order of words (\"adjacency\") are not supported, so they are generated\n",
    "1. Hierarchical relationship of objects in rules looks not supported (i.e. input to rules are bare words, not objects), but it needs to be checked\n",
    "1. We can match word not only literally or by normal form, but also by POS, regex, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903ddd3-5500-4074-b71d-63cc475897ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
