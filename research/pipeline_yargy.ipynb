{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108f30ca-0c7a-4e9a-a5b8-ff293ef4851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(NOTEBOOK_DIR, '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e368ffd0-578b-474a-aeae-d75e83834bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import copy\n",
    "import collections\n",
    "\n",
    "import IPython\n",
    "from yargy.tokenizer import Tokenizer as YrgTokenizer\n",
    "from yargy.interpretation import fact as yrg_fact, attribute as yrg_attr\n",
    "from yargy.pipelines import morph_pipeline as yrg_morph_pipeline\n",
    "from yargy import rule as yrg_rule, or_ as yrg_r_or, and_ as yrg_r_and\n",
    "from yargy.predicates import \\\n",
    "    eq as yrg_rp_eq, gte as yrg_rp_gte, lte as yrg_rp_lte, type as yrg_rp_type, caseless as yrg_rp_caseless, \\\n",
    "    in_caseless as yrg_rp_in_caseless, custom as yrg_rp_custom, normalized as yrg_rp_normalized\n",
    "from yargy import Parser as YrgParser\n",
    "import razdel\n",
    "import navec\n",
    "import slovnet\n",
    "from ipymarkup import show_span_ascii_markup as natasha_show_markup\n",
    "import rdflib\n",
    "from tqdm import tqdm\n",
    "import pymorphy3\n",
    "\n",
    "from utils import dataset_utils\n",
    "from utils import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec58a56f-5637-4e6d-85a0-e174da718d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663c530-9dcf-4126-bbbb-b85faa2943db",
   "metadata": {},
   "source": [
    "# Search by word ontologies with Yargy parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95bd883-a975-4be6-bae8-6dbf2a182135",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61cf9941-1940-4e1c-bc84-0333f1f4677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUESTS_FILE = \"../data/request_db.txt\"\n",
    "ADS_FILE = \"../data/ads_db.txt\"\n",
    "MATCHING_FILE = \"../data/matching_db.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f51ca95-b069-496f-8806-8716bc2e30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ADS_FILE, encoding=\"utf-8\") as f:\n",
    "    ads_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b56949b0-0bb4-40f3-a4df-37cee9ff1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(REQUESTS_FILE, encoding=\"utf-8\") as f:\n",
    "    requests_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "357b3da0-cecd-4312-8bb8-d78563d1e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_markup = dataset_utils.load_matching_data(MATCHING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397f814-7690-447a-819a-d64a49dc317e",
   "metadata": {},
   "source": [
    "## Constructing Ontologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d987674c-3b4e-4a8a-94a8-4f0785448288",
   "metadata": {},
   "source": [
    "### Service Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "556804f0-b281-4d30-ba85-d54e1a46f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "MORPH_AN = pymorphy3.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7b54d07-8e27-4765-a4f6-8e23f5a9ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ListTokenizer(YrgTokenizer):\n",
    "#     def __init__(self, morph):\n",
    "#         super(ListTokenizer, self).__init__([])\n",
    "#         self.morph = morph\n",
    "\n",
    "#     def __call__(self, tokens):\n",
    "#         for tok in tokens:\n",
    "#             yield YrgToken(value=self.morph.parse(tok)[0].normal_form, span=, type=)\n",
    "\n",
    "\n",
    "def create_rule_obj_w_attrs(o_obj, n_obj, adj_dict):\n",
    "\n",
    "    def _get_rule_first_term(adj_rule):\n",
    "        if adj_rule.__class__.__name__ == \"PipelineRule\":\n",
    "            return adj_rule.pipeline.lines[0]\n",
    "        elif adj_rule.__class__.__name__ == \"OrRule\":\n",
    "            return adj_rule.rules[0].productions[0].terms[0].value\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown class: {adj_rule.__class__.__name__}\")\n",
    "\n",
    "    # attributing all adjectives to the object\n",
    "    attributed_dict = {\n",
    "        # we have to copy each adjectives, because they are modified when parser is created (MorphPipelineScheme to MorphPipeline),\n",
    "        # but user can potentially pass same adjective instances for multiple objects\n",
    "        prop_name: [copy.deepcopy(adj.interpretation(getattr(o_obj, prop_name).const(_get_rule_first_term(adj)))) for adj in adj_list]\n",
    "        for prop_name, adj_list in adj_dict.items()\n",
    "    }\n",
    "\n",
    "    # generate rules for all word positions of attr adjectives and object noun\n",
    "    rule_variants = []\n",
    "    for perm_item_list in itertools.permutations(list(attributed_dict.keys()) + [n_obj]):\n",
    "        rule_variants.append(\n",
    "            yrg_rule(\n",
    "                *(\n",
    "                    yrg_r_or(*attributed_dict[p_item]).optional() if p_item is not n_obj else p_item\n",
    "                    for p_item in perm_item_list\n",
    "                )\n",
    "            ).interpretation(o_obj)\n",
    "        )\n",
    "    o_attr_variants_proxy_obj = yrg_fact(f\"{o_obj.__name__}_attr_vars_proxy\", [\"value\"])\n",
    "    high_level_or_rule = yrg_r_or(*rule_variants).interpretation(o_attr_variants_proxy_obj.value).interpretation(o_attr_variants_proxy_obj)\n",
    "\n",
    "    return high_level_or_rule\n",
    "\n",
    "\n",
    "def add_object_parser(obj_class_name, obj_noun_list, obj_prop_dict, size_rule, parser_list):\n",
    "    o_obj = yrg_fact(obj_class_name, list(obj_prop_dict.keys()))\n",
    "    n_obj = yrg_morph_pipeline(obj_noun_list)\n",
    "    r_obj = create_rule_obj_w_attrs(\n",
    "        o_obj,\n",
    "        n_obj,\n",
    "        obj_prop_dict,\n",
    "    )\n",
    "    if size_rule is not None:\n",
    "        o_obj_size_proxy = yrg_fact(f\"{obj_class_name}_size_proxy\", [\"main_obj\", \"PARSED_size_info\"])\n",
    "        r_obj = yrg_rule(\n",
    "            r_obj.interpretation(o_obj_size_proxy.main_obj),\n",
    "            yrg_rule(\n",
    "                yrg_rp_eq(\",\").optional(),\n",
    "                copy.deepcopy(size_rule),\n",
    "            ).optional().interpretation(o_obj_size_proxy.PARSED_size_info),\n",
    "        ).interpretation(o_obj_size_proxy)\n",
    "    # parser_list.append(YrgParser(r_obj, tokenizer=ListTokenizer(MORPH_AN)))\n",
    "    parser_list.append(YrgParser(r_obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc16d00f-a595-43d0-a976-54bdd6e6ad1a",
   "metadata": {},
   "source": [
    "### Clothes Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d10bf2c-7860-46b1-afed-f648ed075682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(rdflib.term.URIRef('http://localhost/Jacket'),\n",
       "  rdflib.term.URIRef('http://localhost/has_names'),\n",
       "  rdflib.term.Literal('куртка')),\n",
       " (rdflib.term.URIRef('http://localhost/Jacket'),\n",
       "  rdflib.term.URIRef('http://localhost/has_names'),\n",
       "  rdflib.term.Literal('ветровка')),\n",
       " (rdflib.term.URIRef('http://localhost/Jacket'),\n",
       "  rdflib.term.URIRef('http://localhost/has_names'),\n",
       "  rdflib.term.Literal('бомбер')),\n",
       " (rdflib.term.URIRef('http://localhost/Jacket'),\n",
       "  rdflib.term.URIRef('http://localhost/has_names'),\n",
       "  rdflib.term.Literal('куртка-бомбер')),\n",
       " (rdflib.term.URIRef('http://localhost/Jacket'),\n",
       "  rdflib.term.URIRef('http://localhost/has_names'),\n",
       "  rdflib.term.Literal('летная куртка')),\n",
       " (rdflib.term.URIRef('http://localhost/Jacket'),\n",
       "  rdflib.term.URIRef('http://localhost/has_names'),\n",
       "  rdflib.term.Literal('куртка летная'))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_ontology = \"\"\"\n",
    "@prefix local: <http://localhost/> .\n",
    "\n",
    "local:outer_wear local:is_subclass local:clothing .\n",
    "\n",
    "local:clothing\n",
    "    local:is_included local:parsed_objects ;\n",
    "    local:has_names \"вещь\" .\n",
    "\n",
    "local:outer_wear\n",
    "    local:is_included local:parsed_objects ;\n",
    "    local:has_names \"одежда\" .\n",
    "\n",
    "local:Coat\n",
    "    local:is_included local:parsed_objects ;\n",
    "    local:is_subclass local:outer_wear ;\n",
    "    local:has_names \"пальто\", \"полупальто\" .\n",
    "local:Jacket\n",
    "    local:is_included local:parsed_objects ;\n",
    "    local:is_subclass local:outer_wear ;\n",
    "    local:has_names \"куртка\", \"ветровка\", \"бомбер\", \"куртка-бомбер\", \"летная куртка\", \"куртка летная\" .\n",
    "local:Sweater\n",
    "    local:is_included local:parsed_objects ;\n",
    "    local:is_subclass local:outer_wear ;\n",
    "    local:has_names \"кофта\", \"свитер\" .\n",
    "local:Blouse\n",
    "    local:is_included local:parsed_objects ;\n",
    "    local:is_subclass local:outer_wear ;\n",
    "    local:has_names \"блузка\" .\n",
    "local:Trousers\n",
    "    local:is_included local:parsed_objects ;\n",
    "    local:is_subclass local:outer_wear ;\n",
    "    local:has_names \"штаны\", \"джинсы\" .\n",
    "local:Skirt\n",
    "    local:is_included local:parsed_objects ;\n",
    "    local:is_subclass local:outer_wear ;\n",
    "    local:has_names \"юбка\" .\n",
    "local:Shirt\n",
    "    local:is_included local:parsed_objects ;\n",
    "    local:is_subclass local:outer_wear ;\n",
    "    local:has_names \"рубашка\" .\n",
    "\"\"\"\n",
    "ontology_g = rdflib.Graph()\n",
    "ontology_g.parse(data=global_ontology, format=\"turtle\")\n",
    "list(ontology_g.triples((rdflib.term.URIRef(\"http://localhost/Jacket\"), rdflib.term.URIRef(\"http://localhost/has_names\"), None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71bd14f3-8438-4f1b-84d1-709cfd761c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_size_letters(token, max_x_count):\n",
    "    res = True\n",
    "    first_digits = []\n",
    "    letters_started = False\n",
    "    end_letter_reached = False\n",
    "    x_count = 0\n",
    "    for c in token:\n",
    "        if end_letter_reached:\n",
    "            res = False\n",
    "            break\n",
    "        if c.isdigit():\n",
    "            if letters_started:\n",
    "                res = False\n",
    "                break\n",
    "            first_digits.append(c)\n",
    "            continue\n",
    "        if not letters_started:\n",
    "            if len(first_digits) > 0:\n",
    "                if c.lower() != \"x\":\n",
    "                    res = False\n",
    "                    break\n",
    "                digit_val = int(\"\".join(first_digits))\n",
    "                if digit_val < 1 or digit_val > max_x_count:\n",
    "                    res = False\n",
    "                    break\n",
    "            if c.lower() not in [\"x\", \"s\", \"m\", \"l\"]:\n",
    "                res = False\n",
    "                break\n",
    "            if c.lower() in [\"s\", \"m\", \"l\"]:\n",
    "                end_letter_reached = True\n",
    "            first_digits = []\n",
    "            letters_started = True\n",
    "            continue\n",
    "        if c.lower() == \"x\":\n",
    "            x_count += 1\n",
    "            if len(first_digits) > 0 or x_count > max_x_count:\n",
    "                res = False\n",
    "                break\n",
    "            continue\n",
    "        if c.lower() not in [\"s\", \"m\", \"l\"]:\n",
    "            res = False\n",
    "            break\n",
    "        end_letter_reached = True\n",
    "    if not letters_started or not end_letter_reached:\n",
    "        res = False\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2049f09-ee2a-4984-b15f-c7b454618789",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CLOTHES_SIZE_INT = 18\n",
    "MAX_CLOTHES_SIZE_INT = 82\n",
    "MIN_CHILD_CLOTHES_SIZE_INT = MIN_CLOTHES_SIZE_INT\n",
    "MAX_CHILD_CLOTHES_SIZE_INT = 43\n",
    "MIN_W_SCHOOL_CLOTHES_SIZE_INT = 26\n",
    "MAX_W_SCHOOL_CLOTHES_SIZE_INT = 48\n",
    "MIN_M_SCHOOL_CLOTHES_SIZE_INT = 28\n",
    "MAX_M_SCHOOL_CLOTHES_SIZE_INT = 50\n",
    "MAX_CLOTHES_SIZE_X_COUNT = 12\n",
    "\n",
    "MAIN_M_GENDER_NAME_STR = \"мужской\"\n",
    "MAIN_W_GENDER_NAME_STR = \"женский\"\n",
    "\n",
    "rule_parsers = []\n",
    "\n",
    "# === general attributes ===\n",
    "\n",
    "gen_attributes = {}\n",
    "\n",
    "gen_attributes[\"gender\"] = [\n",
    "    yrg_r_or(\n",
    "        yrg_rule(yrg_rp_normalized(MAIN_M_GENDER_NAME_STR)),\n",
    "        yrg_rule(\n",
    "            yrg_rp_caseless(\"муж\"),\n",
    "            yrg_rp_eq(\".\").optional(),\n",
    "        ),\n",
    "    ),\n",
    "    yrg_r_or(\n",
    "        yrg_rule(yrg_rp_normalized(MAIN_W_GENDER_NAME_STR)),\n",
    "        yrg_rule(\n",
    "            yrg_rp_caseless(\"жен\"),\n",
    "            yrg_rp_eq(\".\").optional(),\n",
    "        ),\n",
    "    ),\n",
    "    yrg_morph_pipeline([\n",
    "        \"унисекс\",\n",
    "        \"юнисекс\",\n",
    "    ]),\n",
    "]\n",
    "\n",
    "gen_attributes[\"season\"] = [\n",
    "    yrg_morph_pipeline([\n",
    "        \"демсезон\",\n",
    "        \"демисезон\",\n",
    "        \"демисезонный\",\n",
    "        \"весенний\",\n",
    "        \"весна\",\n",
    "        \"осенний\",\n",
    "        \"осень\",\n",
    "        \"весна-осень\",\n",
    "        \"осень-весна\",\n",
    "    ]),\n",
    "    yrg_morph_pipeline([\n",
    "        \"зимний\",\n",
    "        \"зима\",\n",
    "        \"зим\",\n",
    "        \"зимн\",\n",
    "    ]),\n",
    "    yrg_morph_pipeline([\n",
    "        \"летний\",\n",
    "        \"лето\",\n",
    "        \"лет\",\n",
    "        \"летн\",\n",
    "    ]),\n",
    "]\n",
    "\n",
    "gen_attributes[\"material\"] = [\n",
    "    yrg_morph_pipeline([\n",
    "        \"джинсовый\",\n",
    "        \"джинса\",\n",
    "    ]),\n",
    "    yrg_morph_pipeline([\n",
    "        \"кожаный\",\n",
    "        \"кожа\",\n",
    "    ]),\n",
    "    yrg_morph_pipeline([\n",
    "        \"синтепоновый\",\n",
    "        \"синтепон\",\n",
    "    ]),\n",
    "]\n",
    "\n",
    "# === indirect size and gender information ===\n",
    "\n",
    "o_size_indirect_info = yrg_fact(\n",
    "    \"size_indirect_info\", [\"keyword\", \"year_info_from_y\", \"year_info_from_m\", \"year_info_to_y\", \"year_info_to_m\"]\n",
    ")\n",
    "r_size_gender_indirect_info = yrg_rule(\n",
    "    yrg_r_or(\n",
    "        yrg_rp_caseless(\"на\"),\n",
    "        yrg_rp_caseless(\"для\"),\n",
    "    ).optional(),\n",
    "    yrg_morph_pipeline([\n",
    "        \"мальчик\",\n",
    "        \"девочка\",\n",
    "        \"мужчина\",\n",
    "        \"женщина\",\n",
    "        \"ребёнок\",\n",
    "        \"взрослый\",\n",
    "        \"школьник\",\n",
    "        \"школьница\",\n",
    "    ]).interpretation(o_size_indirect_info.keyword.normalized()),\n",
    ")\n",
    "r_size_year_info = yrg_r_or(\n",
    "    yrg_rule(\n",
    "        yrg_rp_type(\"INT\").interpretation(o_size_indirect_info.year_info_from_y),\n",
    "        yrg_rule(\n",
    "            yrg_rp_eq(\"-\"),\n",
    "            yrg_rp_type(\"INT\").interpretation(o_size_indirect_info.year_info_to_y)\n",
    "        ).optional(),\n",
    "        yrg_morph_pipeline([\"лет\", \"год\"]),\n",
    "    ),\n",
    "    yrg_rule(\n",
    "        yrg_rp_type(\"INT\").interpretation(o_size_indirect_info.year_info_from_m),\n",
    "        yrg_rule(\n",
    "            yrg_rp_eq(\"-\"),\n",
    "            yrg_rp_type(\"INT\").interpretation(o_size_indirect_info.year_info_to_m)\n",
    "        ).optional(),\n",
    "        yrg_morph_pipeline([\"месяц\", \"мес\"]),\n",
    "    ),\n",
    ").interpretation(o_size_indirect_info)\n",
    "r_size_year_gender_indirect_info = yrg_rule(\n",
    "    r_size_gender_indirect_info,\n",
    "    r_size_year_info.optional(),\n",
    ").interpretation(o_size_indirect_info)\n",
    "\n",
    "# === direct size and gender information ===\n",
    "\n",
    "o_size_number = yrg_fact(\"size_number\", [\"int_part\", \"frac_part\"])\n",
    "r_size_number = yrg_rule(\n",
    "    yrg_r_and(\n",
    "        yrg_rp_gte(MIN_CLOTHES_SIZE_INT),\n",
    "        yrg_rp_lte(MAX_CLOTHES_SIZE_INT),\n",
    "    ).interpretation(o_size_number.int_part),\n",
    "    yrg_r_or(\n",
    "        yrg_rule(\n",
    "            yrg_rp_eq(\".\"),\n",
    "            yrg_rp_type(\"INT\").interpretation(o_size_number.frac_part),\n",
    "        ),\n",
    "        yrg_rule(\n",
    "            yrg_rp_caseless(\"с\"),\n",
    "            yrg_rp_caseless(\"половиной\")\n",
    "        ).interpretation(o_size_number.frac_part.const(\"5\")),\n",
    "    ).optional(),\n",
    ").interpretation(o_size_number)\n",
    "o_size_number_list = yrg_fact(\"size_number_list\", [\"from_info\", \"to_info\"])\n",
    "r_size_number_list = yrg_rule(\n",
    "    r_size_number.interpretation(o_size_number_list.from_info),\n",
    "    yrg_rule(\n",
    "        yrg_rp_eq(\"-\"),  # all types of dashes are converted to \"-\" on preprocessing\n",
    "        r_size_number.interpretation(o_size_number_list.to_info),\n",
    "    ).optional(),\n",
    ").interpretation(o_size_number_list)\n",
    "\n",
    "o_size_letters = yrg_fact(\"size_letters\", [\"letters\"])\n",
    "r_size_letters = yrg_rule(\n",
    "    yrg_r_and(   # tokenizer splits numbers from letters, so 10XL becomes '10', 'XL'\n",
    "        yrg_rp_gte(2),\n",
    "        yrg_rp_lte(MAX_CLOTHES_SIZE_X_COUNT),\n",
    "    ).optional(),\n",
    "    yrg_rp_custom(lambda tok: is_size_letters(tok, MAX_CLOTHES_SIZE_X_COUNT)),\n",
    ").interpretation(o_size_letters.letters).interpretation(o_size_letters)\n",
    "o_size_letters_list = yrg_fact(\"size_letters_list\", [\"from_info\", \"to_info\"])\n",
    "r_size_letters_list = yrg_rule(\n",
    "    r_size_letters.interpretation(o_size_letters_list.from_info),\n",
    "    yrg_rule(\n",
    "        yrg_rp_eq(\"-\"),  # all types of dashes are converted to \"-\" on preprocessing\n",
    "        r_size_letters.interpretation(o_size_letters_list.to_info),\n",
    "    ).optional(),\n",
    ").interpretation(o_size_letters_list)\n",
    "\n",
    "n_size_word = yrg_r_or(\n",
    "    yrg_rule(yrg_rp_normalized(\"размер\")),\n",
    "    yrg_rule(\n",
    "        yrg_rp_caseless(\"р\"),\n",
    "        yrg_rp_eq(\".\").optional()\n",
    "    ),\n",
    ")\n",
    "o_size_direct_values = yrg_fact(\"size_direct_values\", [\"direct_values\"])\n",
    "r_size_direct_values = yrg_r_or(\n",
    "    yrg_rule(\n",
    "        n_size_word.optional(),\n",
    "        yrg_r_or(\n",
    "            r_size_number_list,\n",
    "            r_size_letters_list,\n",
    "        ).interpretation(o_size_direct_values.direct_values),\n",
    "    ),\n",
    "    yrg_rule(\n",
    "        r_size_number_list,\n",
    "        n_size_word,\n",
    "    ).interpretation(o_size_direct_values.direct_values),\n",
    ").interpretation(o_size_direct_values)\n",
    "\n",
    "# === general size information ===\n",
    "\n",
    "o_size_info = yrg_fact(\"size_info\", [\"direct_values\", \"indirect_values\"])\n",
    "r_size_info = yrg_r_or(\n",
    "    r_size_year_gender_indirect_info.interpretation(o_size_info.indirect_values),\n",
    "    r_size_direct_values.interpretation(o_size_info.direct_values),\n",
    ").interpretation(o_size_info)\n",
    "\n",
    "# === objects ===\n",
    "\n",
    "add_object_parser(\n",
    "    obj_class_name=\"Coat\",\n",
    "    obj_noun_list=[\n",
    "        \"пальто\",\n",
    "        \"полупальто\",\n",
    "    ],\n",
    "    obj_prop_dict={\n",
    "        **gen_attributes,\n",
    "    },\n",
    "    size_rule=r_size_info,\n",
    "    parser_list=rule_parsers,\n",
    ")\n",
    "\n",
    "add_object_parser(\n",
    "    obj_class_name=\"Jacket\",\n",
    "    obj_noun_list=[\n",
    "        \"куртка\",\n",
    "        \"ветровка\",\n",
    "        \"бомбер\",\n",
    "        \"куртка-бомбер\",\n",
    "        \"летная куртка\",\n",
    "        \"куртка летная\",\n",
    "    ],\n",
    "    obj_prop_dict={\n",
    "        **gen_attributes,\n",
    "    },\n",
    "    size_rule=r_size_info,\n",
    "    parser_list=rule_parsers,\n",
    ")\n",
    "\n",
    "add_object_parser(\n",
    "    obj_class_name=\"Sweater\",\n",
    "    obj_noun_list=[\n",
    "        \"кофта\",\n",
    "        \"свитер\",\n",
    "    ],\n",
    "    obj_prop_dict={\n",
    "        **gen_attributes,\n",
    "    },\n",
    "    size_rule=r_size_info,\n",
    "    parser_list=rule_parsers,\n",
    ")\n",
    "\n",
    "add_object_parser(\n",
    "    obj_class_name=\"Blouse\",\n",
    "    obj_noun_list=[\n",
    "        \"блузка\",\n",
    "    ],\n",
    "    obj_prop_dict={\n",
    "        **{k: v for k, v in gen_attributes.items() if k != \"gender\"},  # it is supposed that blouses are only for women\n",
    "    },\n",
    "    size_rule=r_size_info,\n",
    "    parser_list=rule_parsers,\n",
    ")\n",
    "\n",
    "add_object_parser(\n",
    "    obj_class_name=\"Trousers\",\n",
    "    obj_noun_list=[\n",
    "        \"штаны\",\n",
    "        \"джинсы\",\n",
    "    ],\n",
    "    obj_prop_dict={\n",
    "        **gen_attributes,\n",
    "    },\n",
    "    size_rule=r_size_info,\n",
    "    parser_list=rule_parsers,\n",
    ")\n",
    "\n",
    "add_object_parser(\n",
    "    obj_class_name=\"Skirt\",\n",
    "    obj_noun_list=[\n",
    "        \"юбка\",\n",
    "    ],\n",
    "    obj_prop_dict={\n",
    "        **{k: v for k, v in gen_attributes.items() if k != \"gender\"},  # it is supposed that skirts are only for women\n",
    "    },\n",
    "    size_rule=r_size_info,\n",
    "    parser_list=rule_parsers,\n",
    ")\n",
    "\n",
    "add_object_parser(\n",
    "    obj_class_name=\"Shirt\",\n",
    "    obj_noun_list=[\n",
    "        \"рубашка\",\n",
    "    ],\n",
    "    obj_prop_dict={\n",
    "        **gen_attributes,\n",
    "    },\n",
    "    size_rule=r_size_info,\n",
    "    parser_list=rule_parsers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30ae56c5-e185-4e25-b1f1-5591b1a55cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_class_name = \"tst_jacket\"\n",
    "# obj_prop_dict = gen_attributes\n",
    "# obj_noun_list = [\"куртка\"]\n",
    "# o_obj = yrg_fact(obj_class_name, list(obj_prop_dict.keys()))\n",
    "# n_obj = yrg_morph_pipeline(obj_noun_list)\n",
    "# r_obj = create_rule_obj_w_attrs(\n",
    "#     o_obj,\n",
    "#     n_obj,\n",
    "#     obj_prop_dict,\n",
    "# )\n",
    "\n",
    "# size_rule = r_size_info\n",
    "# o_obj_size_proxy = yrg_fact(f\"{obj_class_name}_size_proxy\", [\"main_obj\", \"PARSED_size_info\"])\n",
    "# r_obj = yrg_rule(\n",
    "#     r_obj.interpretation(o_obj_size_proxy.main_obj),\n",
    "#     copy.deepcopy(size_rule).interpretation(o_obj_size_proxy.PARSED_size_info),\n",
    "#     # yrg_rule(\n",
    "#     #     yrg_rp_eq(\",\").optional(),\n",
    "#     #     copy.deepcopy(size_rule),\n",
    "#     # ).optional().interpretation(o_obj_size_proxy.PARSED_size_info),\n",
    "# ).interpretation(o_obj_size_proxy)\n",
    "\n",
    "# parser = YrgParser(r_obj)\n",
    "# matches = parser.findall(\"8. Кожаная куртка р. 40 на фото\")\n",
    "# for m in matches:\n",
    "#     print(m)\n",
    "#     print(m.fact)\n",
    "#     # print(f\"{m.tree.root.production}\")\n",
    "#     # print(f\"{m.tree.root.production.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b6c62e3-409e-49a1-81a2-3b51da82a715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rules for 7 objects were created\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rules for {len(rule_parsers)} objects were created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "580dbe5f-86a8-4855-8010-8dd283299c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://storage.yandexcloud.net/natasha-navec/packs/navec_news_v1_1B_250K_300d_100q.tar\n",
    "# https://storage.yandexcloud.net/natasha-slovnet/packs/slovnet_syntax_news_v1.tar\n",
    "\n",
    "TOKENIZER = navec.Navec.load('navec_news_v1_1B_250K_300d_100q.tar')  # this model is hardcoded for slovnet syntax analyzer\n",
    "SYNTAX_AN = slovnet.Syntax.load('slovnet_syntax_news_v1.tar')\n",
    "_ = SYNTAX_AN.navec(TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a8150ea-5c6a-4d8d-9ca2-f181fb16cdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://localhost/outer_wear']\n"
     ]
    }
   ],
   "source": [
    "res = ontology_g.query(\"\"\"\n",
    "SELECT DISTINCT ?main_obj\n",
    "WHERE {\n",
    "    ?main_obj local:is_included local:parsed_objects .\n",
    "    ?main_obj local:has_names \"одежда\" .\n",
    "}\n",
    "\"\"\")\n",
    "obj_name_list = [row[0].toPython() for row in res]\n",
    "print(obj_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a31af485-9dd6-4975-8442-324918d99f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_ud_word_relations(text, syntax_an):\n",
    "    # The list below is taken form \"slovnet_syntax_news_v1.tar/vocabs/rel.gz\". Meanings of dependency tags can be taken from\n",
    "    # https://universaldependencies.org/u/dep/index.html (universal), https://universaldependencies.org/ru/dep/index.html (Russian),\n",
    "    # and https://ruscorpora.ru/media/uploads/2023/12/29/rajadw.pdf (Russian UD in Russian).\n",
    "    # <pad>\n",
    "    # acl           - [dependency] to_w is a modifier of from_w (not only adjective)\n",
    "    # acl:relcl     - [dependency] to_w is main word in a sentence that modifies from_w\n",
    "    # advcl         - [dependency] to_w is main word for adverbal info to from_w\n",
    "    # advmod        - [dependency] to_w is adverb for from_w\n",
    "    # amod          - [strong dependency] to_w is adjective to from_w\n",
    "    # appos         - [dependency] to_w is a continuation word for from_w (usually to_w goes after from_w)\n",
    "    # aux           - [dependency] to_w is additional word to verb from_w\n",
    "    # aux:pass      - [dependency] to_w is additional word to passive verb from_w\n",
    "    # case          - [dependency] to_w is a preposition to from_w\n",
    "    # cc            - [dependency] to_w is coordinating conjunction to from_w\n",
    "    # ccomp         - [dependency] to_w is dependent predicate clause for from_w\n",
    "    # compound      - [dependency] to_w is dependent part of compaund word, where from_w is main part (similar to appos?)\n",
    "    # conj          - [equality]   to_w is next list element for from_w (in left to right order)\n",
    "    # cop           - [dependency] to_w is a verb \"to be\" for main word from_w\n",
    "    # csubj         - [dependency] to_w is clausal subject (infinitive verb) to from_w\n",
    "    # csubj:pass    - [dependency] to_w is clausal passive subject to from_w\n",
    "    # dep           - [???]        to_w has unspecified dependency from from_w\n",
    "    # det           - [dependency] to_w is adjective-pronoun for from_w\n",
    "    # discourse     - [dependency] to_w is emotional modification for from_w\n",
    "    # dislocated    - [dependency] to_w is continuation (or generalization) of topic, started by from_w\n",
    "    # expl          - [dependency] to_w is \"this\" or \"that\" that is connected to from_w\n",
    "    # fixed         - [dependency] to_w is continuation of multiword expression after from_w\n",
    "    # flat          - [dependency] to_w is continuation of date or other continuing expression, after from_w\n",
    "    # flat:foreign  - [dependency] to_w is continuation of expression in forreign language, started by from_w\n",
    "    # flat:name     - [dependency] to_w is continuation of name, after from_w\n",
    "    # goeswith      - [dependency] to_w is continuation of a single word that was split by error or intentionlly, where from_w is first part\n",
    "    # iobj          - [dependency] to_w is a dependecy of a verb from_w (in case the verb has more than one dependency)\n",
    "    # list          - [equality]   to_w is a next list item after from_w\n",
    "    # mark          - [dependency] to_w is dependent conjunction to the main word of clause, which is from_w\n",
    "    # nmod          - [dependency] to_w is dependent word that modifies from_w\n",
    "    # nsubj         - [identity]   to_w is nominative subject, from_w usually is a verb (but can be other part of speech)\n",
    "    # nsubj:pass    - [identity]   to_w is nominative subject, from_w a verb in passive form\n",
    "    # nummod        - [dependency] to_w is numerical modifier of from_w\n",
    "    # nummod:entity - [dependency] to_w is \"number sign\" or some identifier of from_w, more details are here:\n",
    "    #                              https://universaldependencies.org/treebanks/ru_syntagrus/ru_syntagrus-dep-nummod-entity.html\n",
    "    # nummod:gov    - [dependency] to_w is numerical modifier for from_w, which is nummod\n",
    "    # obj           - [dependency] to_w is 2nd argument of predicate (usually, a noun, that is dependent from a verb) of from_w\n",
    "    # obl           - [dependency] to_w is nominal modifier of from_w\n",
    "    # obl:agent     - [dependency] to_w answers to \"by whom\", while verb is from_w\n",
    "    # orphan        - [dependency] to_w is contextually connected word to from_w\n",
    "    # parataxis     - [generalization] to_w is explanation to from_w\n",
    "    # punct         - [dependency] to_w is punctuation mark, from_w is main related word, more details are here:\n",
    "    #                 https://universaldependencies.org/treebanks/ru_syntagrus/ru_syntagrus-dep-punct.html\n",
    "    # root          - [identity]   to_w is main explanatory word in sentence (not subject), from_w is not applicable (-1)\n",
    "    # vocative      - [dependency] to_w is name of the person, described by from_w\n",
    "    # xcomp         - [dependency] to_w is auxuliary argument to verb, which is from_w\n",
    "    s_toks = []\n",
    "    for sentence in razdel.sentenize(text):\n",
    "        s_toks.append(list(tok.text for tok in razdel.tokenize(sentence.text)))\n",
    "\n",
    "    relation_list = []\n",
    "    token_idx_offset = 0\n",
    "    for sent_idx, markup in enumerate(syntax_an.map(s_toks)):\n",
    "        for mtok in markup.tokens:\n",
    "            from_idx = int(mtok.head_id) - 1\n",
    "            if from_idx >= 0:\n",
    "                # for \"root\" dependency it is -1\n",
    "                from_idx += token_idx_offset\n",
    "            relation_list.append(\n",
    "                {\"rel\": mtok.rel, \"from\": from_idx, \"to\": int(mtok.id) - 1 + token_idx_offset}\n",
    "            )\n",
    "        token_idx_offset += len(s_toks[sent_idx])\n",
    "\n",
    "    all_toks = []\n",
    "    sentence_ranges = []\n",
    "    sent_offset = 0\n",
    "    for toks in s_toks:\n",
    "        all_toks += toks\n",
    "        sentence_ranges.append((sent_offset, sent_offset + len(toks)))\n",
    "        sent_offset += len(toks)\n",
    "\n",
    "    return relation_list, all_toks, sentence_ranges\n",
    "\n",
    "\n",
    "def _get_ont_hierarchy(ont):\n",
    "    res = ont.query(\n",
    "        \"SELECT DISTINCT ?names \"\n",
    "        \"WHERE { \"\n",
    "        \"    ?main_obj local:is_included local:parsed_objects . \"\n",
    "        \"    ?main_obj local:has_names ?names . \"\n",
    "        \"    FILTER (NOT EXISTS {?main_obj local:is_subclass ?parent_obj .}) \"\n",
    "        \"}\"\n",
    "    )\n",
    "    root_name_list = [row[0].toPython() for row in res]\n",
    "    processed_idx_list = [0]\n",
    "    next_level = 1\n",
    "    class_h_map = [(root_name, []) for root_name in root_name_list]\n",
    "    while True:\n",
    "        parent = class_h_map\n",
    "        child = parent[processed_idx_list[0]]\n",
    "        for level in range(1, next_level):\n",
    "            parent = child[1]\n",
    "            child = parent[processed_idx_list[level]]\n",
    "        child_name = child[0]\n",
    "        assert isinstance(child_name, str)\n",
    "        res = ont.query(\n",
    "            \"SELECT DISTINCT ?names \"\n",
    "            \"WHERE { \"\n",
    "            \"    ?main_obj local:is_included local:parsed_objects . \"\n",
    "            \"    ?main_obj local:has_names ?names . \"\n",
    "            \"    ?main_obj local:is_subclass ?parent_obj . \"\n",
    "            f\"    ?parent_obj local:has_names \\\"{child_name}\\\" . \"\n",
    "            \"}\"\n",
    "        )\n",
    "        grand_childs = [row[0].toPython() for row in res]\n",
    "        if len(grand_childs) > 0:\n",
    "            parent[processed_idx_list[next_level - 1]] = (child[0], [(gc_name, []) for gc_name in grand_childs])\n",
    "            next_level += 1\n",
    "            processed_idx_list.append(0)\n",
    "        else:\n",
    "            if processed_idx_list[next_level - 1] < len(parent) - 1:\n",
    "                processed_idx_list[next_level - 1] += 1\n",
    "            else:\n",
    "                if next_level == 1:\n",
    "                    break\n",
    "                while True:\n",
    "                    del processed_idx_list[next_level - 1]\n",
    "                    next_level -= 1\n",
    "                    parent = class_h_map\n",
    "                    child = parent[processed_idx_list[0]]\n",
    "                    for level in range(1, next_level):\n",
    "                        parent = child[1]\n",
    "                        child = parent[processed_idx_list[level]]\n",
    "                    if processed_idx_list[next_level - 1] < len(parent) - 1 or next_level == 1:\n",
    "                        break\n",
    "                if processed_idx_list[next_level - 1] >= len(parent) - 1:\n",
    "                    break\n",
    "                processed_idx_list[next_level - 1] += 1\n",
    "\n",
    "    flat_hierarchy = []\n",
    "    processed_idx_list = [0]\n",
    "    while True:\n",
    "        parent = class_h_map\n",
    "        level = 0\n",
    "        child = parent[processed_idx_list[level]]\n",
    "        history = [child[0]]\n",
    "        while len(child[1]) > 0:\n",
    "            parent = child[1]\n",
    "            level += 1\n",
    "            if level > len(processed_idx_list) - 1:\n",
    "                processed_idx_list.append(0)\n",
    "            child = parent[processed_idx_list[level]]\n",
    "            history.append(child[0])\n",
    "        flat_hierarchy.append(history.copy())\n",
    "\n",
    "        if processed_idx_list[-1] < len(parent) - 1:\n",
    "            processed_idx_list[-1] += 1\n",
    "        else:\n",
    "            if len(processed_idx_list) == 1:\n",
    "                break\n",
    "            while True:\n",
    "                del processed_idx_list[-1]\n",
    "                parent = class_h_map\n",
    "                child = parent[processed_idx_list[0]]\n",
    "                for level in range(1, len(processed_idx_list)):\n",
    "                    parent = child[1]\n",
    "                    child = parent[processed_idx_list[level]]\n",
    "                if processed_idx_list[-1] < len(parent) - 1 or len(processed_idx_list) == 1:\n",
    "                    break\n",
    "            if processed_idx_list[-1] >= len(parent) - 1:\n",
    "                break\n",
    "            processed_idx_list[-1] += 1\n",
    "    del class_h_map\n",
    "\n",
    "    distinct_names = set()\n",
    "    for hierarchy_line in flat_hierarchy:\n",
    "        distinct_names |= set(hierarchy_line)\n",
    "    name_obj_map = {}\n",
    "    for obj_name in sorted(distinct_names):\n",
    "        res = ont.query(\n",
    "            \"SELECT ?main_obj \"\n",
    "            \"WHERE { \"\n",
    "            \"    ?main_obj local:is_included local:parsed_objects . \"\n",
    "            f\"    ?main_obj local:has_names \\\"{obj_name}\\\" . \"\n",
    "            \"}\"\n",
    "        )\n",
    "        out_list = [row[0].toPython() for row in res]\n",
    "        assert len(out_list) == 1\n",
    "        name_obj_map[obj_name] = out_list[0]\n",
    "\n",
    "    return flat_hierarchy, name_obj_map\n",
    "\n",
    "\n",
    "def _get_all_word_relations(text, ont, morph_an, syntax_an, size_rule):\n",
    "    word_hierarchy_list, name_obj_map = _get_ont_hierarchy(ont)\n",
    "    relation_list, toks, sentence_ranges = _get_ud_word_relations(text, syntax_an)\n",
    "\n",
    "    size_parser = YrgParser(size_rule)\n",
    "    matches = size_parser.findall(text)\n",
    "    for m in matches:\n",
    "        pos = 0\n",
    "        is_size_found = False\n",
    "        tok_idx = 0\n",
    "        while tok_idx < len(toks):\n",
    "            tok = toks[tok_idx]\n",
    "            pos = text.find(tok, pos)\n",
    "            assert pos >= 0\n",
    "            if (pos >= m.span.start and pos < m.span.stop) or (pos + len(tok) >= m.span.stop and not is_size_found):\n",
    "                relation_list.append({\"rel\": \"ont:size\", \"from\": tok_idx, \"to\": tok_idx})\n",
    "                is_size_found = True\n",
    "            else:\n",
    "                if is_size_found:\n",
    "                    # workaround for size ranges, because rule always selects shortest match span and ranges like \"80-90\" become \"80\"\n",
    "                    if tok == \"-\":\n",
    "                        relation_list.append({\"rel\": \"ont:size\", \"from\": tok_idx, \"to\": tok_idx})\n",
    "                        tok_idx += 1\n",
    "                        if tok_idx < len(toks) and toks[tok_idx].isdigit():\n",
    "                            relation_list.append({\"rel\": \"ont:size\", \"from\": tok_idx, \"to\": tok_idx})\n",
    "                            tok_idx += 1\n",
    "                    break\n",
    "            tok_idx += 1\n",
    "        assert is_size_found\n",
    "\n",
    "    normed_toks = [morph_an.parse(tok)[0].normal_form for tok in toks]\n",
    "    for tok_idx, tok in enumerate(normed_toks):\n",
    "        processed_h_terms = set()\n",
    "        for hierarchy_line in word_hierarchy_list:\n",
    "            if tok in hierarchy_line:\n",
    "                if tok not in processed_h_terms:\n",
    "                    processed_h_terms.add(tok)\n",
    "                    relation_list.append({\"rel\": f\"ont:obj:{name_obj_map[tok]}\", \"from\": tok_idx, \"to\": tok_idx})\n",
    "                h_idx = hierarchy_line.index(tok)\n",
    "                if h_idx < len(hierarchy_line) - 1:\n",
    "                    child_names = hierarchy_line[h_idx + 1:]\n",
    "                    for ch_tok_idx, ch_tok in enumerate(normed_toks):\n",
    "                        if ch_tok in child_names:\n",
    "                            relation_list.append({\"rel\": \"ont:rel:inst\", \"from\": tok_idx, \"to\": ch_tok_idx})\n",
    "\n",
    "    return relation_list, toks, sentence_ranges\n",
    "\n",
    "\n",
    "def split_text_for_rules(text, ont, morph_an, syntax_an, size_rule):\n",
    "    IDENTITY_DEPS = [\"nsubj\", \"nsubj:pass\", \"root\"]\n",
    "    EQUALITY_DEPS = [\"conj\", \"list\"]\n",
    "    STRONG_DEPS = [\"amod\"]\n",
    "\n",
    "    def _infer_macro_relations(rel_list, sentence_ranges):\n",
    "        macro_rels = []\n",
    "\n",
    "        # same sentence\n",
    "        for sent_range in sentence_ranges:\n",
    "            sent_idx_list = list(range(sent_range[0], sent_range[1]))\n",
    "            size_info_cnt = 0\n",
    "            obj_cnt = 0\n",
    "            tok_type = None\n",
    "            first_type = None\n",
    "            last_size_info_idx = -1\n",
    "            for idx in sent_idx_list:\n",
    "                idx_rels = [rel[\"rel\"] for rel in rel_list if rel[\"to\"] == idx]\n",
    "                if \"ont:size\" in idx_rels:\n",
    "                    if tok_type != 'size':  # size info can contain multiple tokens\n",
    "                        size_info_cnt += 1\n",
    "                    tok_type = 'size'\n",
    "                    last_size_info_idx = idx\n",
    "                elif any(rel.startswith(\"ont:obj:\") for rel in idx_rels):  # any() returns False on empty input\n",
    "                    obj_cnt += 1  # object is identified by single token\n",
    "                    tok_type = 'obj'\n",
    "                else:\n",
    "                    tok_type = None\n",
    "                if first_type is None and tok_type is not None:\n",
    "                    first_type = tok_type\n",
    "    \n",
    "            if size_info_cnt > 0 and obj_cnt > 0:\n",
    "                last_size_info_start_idx = None\n",
    "                last_assign_tok_idx = 0\n",
    "                is_size_info_continues = False\n",
    "                if first_type == 'obj':\n",
    "                    for idx_idx, idx in enumerate(sent_idx_list):\n",
    "                        idx_rels = [rel[\"rel\"] for rel in rel_list if rel[\"to\"] == idx]\n",
    "                        if \"ont:size\" in idx_rels:\n",
    "                            if not is_size_info_continues:\n",
    "                                last_size_info_start_idx = idx\n",
    "                            for obj_idx in sent_idx_list[last_assign_tok_idx:idx_idx]:\n",
    "                                obj_idx_rels = [rel[\"rel\"] for rel in rel_list if rel[\"to\"] == obj_idx]\n",
    "                                if any(rel.startswith(\"ont:obj:\") for rel in obj_idx_rels):  # any() returns False on empty input\n",
    "                                    macro_rels.append({\"rel\": \"size\", \"from\": idx, \"to\": obj_idx})\n",
    "                            is_size_info_continues = True\n",
    "                        else:\n",
    "                            if is_size_info_continues:\n",
    "                                last_assign_tok_idx = idx_idx\n",
    "                            is_size_info_continues = False\n",
    "                        if idx > last_size_info_idx and any(rel.startswith(\"ont:obj:\") for rel in idx_rels):\n",
    "                            for size_idx in range(last_size_info_start_idx, last_size_info_idx + 1):\n",
    "                                macro_rels.append({\"rel\": \"size\", \"from\": size_idx, \"to\": idx})\n",
    "                else:\n",
    "                    for idx_idx, idx in enumerate(sent_idx_list):\n",
    "                        idx_rels = [rel[\"rel\"] for rel in rel_list if rel[\"to\"] == idx]\n",
    "                        if any(rel.startswith(\"ont:obj:\") for rel in idx_rels):  # any() returns False on empty input\n",
    "                            is_size_info_continues = False\n",
    "                            for size_idx in sent_idx_list[last_assign_tok_idx:idx_idx]:\n",
    "                                size_idx_rels = [rel[\"rel\"] for rel in rel_list if rel[\"to\"] == size_idx]\n",
    "                                if \"ont:size\" in size_idx_rels:\n",
    "                                    macro_rels.append({\"rel\": \"size\", \"from\": size_idx, \"to\": idx})\n",
    "                        else:\n",
    "                            if \"ont:size\" in idx_rels:\n",
    "                                if not is_size_info_continues:\n",
    "                                    last_assign_tok_idx = idx_idx\n",
    "                                is_size_info_continues = True\n",
    "                        # even if sentence is ended by size info, it is dropped, because all objects were defined by previous size infos\n",
    "\n",
    "        # different sentences\n",
    "        no_size_sent_list = []\n",
    "        size_sent_list = []\n",
    "        for idx in range(len(toks)):\n",
    "            idx_rels = [rel[\"rel\"] for rel in rel_list if rel[\"to\"] == idx]\n",
    "            if any(rel.startswith(\"ont:obj:\") for rel in idx_rels):  # any() returns False on empty input\n",
    "                if not any(mrel[\"to\"] == idx for mrel in macro_rels if mrel[\"rel\"] == \"size\"):\n",
    "                    for sent_idx, sent_range in enumerate(sentence_ranges):\n",
    "                        if idx >= sent_range[0] and idx < sent_range[1]:\n",
    "                            no_size_sent_list.append(sent_idx)\n",
    "                            break\n",
    "            if \"ont:size\" in idx_rels:\n",
    "                for sent_idx, sent_range in enumerate(sentence_ranges):\n",
    "                    if idx >= sent_range[0] and idx < sent_range[1]:\n",
    "                        size_sent_list.append(sent_idx)\n",
    "                        break\n",
    "        if len(size_sent_list) > 0 and len(no_size_sent_list) > 0:\n",
    "            for no_size_sent_idx in no_size_sent_list:\n",
    "                closest_size_sent_idx = min(\n",
    "                    [(abs(size_sent_idx - no_size_sent_idx), size_sent_idx) for size_sent_idx in size_sent_list], key=lambda x: x[0]\n",
    "                )[1]\n",
    "                size_idx_list = [\n",
    "                    idx for idx in list(range(sentence_ranges[closest_size_sent_idx][0], sentence_ranges[closest_size_sent_idx][1]))\n",
    "                    if \"ont:size\" in [rel[\"rel\"] for rel in rel_list if rel[\"to\"] == idx]\n",
    "                ]\n",
    "                for idx in list(range(sentence_ranges[no_size_sent_idx][0], sentence_ranges[no_size_sent_idx][1])):\n",
    "                    idx_rels = [rel[\"rel\"] for rel in rel_list if rel[\"to\"] == idx]\n",
    "                    if any(rel.startswith(\"ont:obj:\") for rel in idx_rels):  # any() returns False on empty input\n",
    "                        # according to the processing above, all objects in sentence are not connected to size info, so no check is needed\n",
    "                        for size_idx in size_idx_list:\n",
    "                            macro_rels.append({\"rel\": \"size\", \"from\": size_idx, \"to\": idx})\n",
    "\n",
    "        return macro_rels\n",
    "\n",
    "\n",
    "    def _extract_isolated_tree(item_idx, rels, toks, only_strong_deps_flag):\n",
    "        excluded_deps = EQUALITY_DEPS\n",
    "        if only_strong_deps_flag:\n",
    "            idx_to_check_list = [item_idx]\n",
    "            found_idx_set = set()\n",
    "            while len(idx_to_check_list) > 0:\n",
    "                current_idx = idx_to_check_list.pop()\n",
    "                dependent_elems = [rel[\"to\"] for rel in rels if rel[\"from\"] == current_idx and rel[\"rel\"] in STRONG_DEPS]\n",
    "                dependent_elems += [rel[\"from\"] for rel in rels if rel[\"to\"] == current_idx and rel[\"rel\"] in STRONG_DEPS]\n",
    "                idx_to_check_list += [idx for idx in dependent_elems if idx not in found_idx_set and idx != current_idx]\n",
    "                found_idx_set.add(current_idx)\n",
    "        else:\n",
    "            idx_to_check_list = [item_idx]\n",
    "            found_idx_set = set()\n",
    "            while len(idx_to_check_list) > 0:\n",
    "                current_idx = idx_to_check_list.pop()\n",
    "                dependent_elems = [rel[\"to\"] for rel in rels if rel[\"from\"] == current_idx and rel[\"rel\"] not in EQUALITY_DEPS]\n",
    "                dependent_elems += [rel[\"from\"] for rel in rels if rel[\"to\"] == current_idx and rel[\"rel\"] not in EQUALITY_DEPS]\n",
    "                idx_to_check_list += [idx for idx in dependent_elems if idx not in found_idx_set and idx != current_idx]\n",
    "                found_idx_set.add(current_idx)\n",
    "        return list(sorted(found_idx_set))\n",
    "\n",
    "    # def _form_sentence_for_list_item(item_idx, rels, toks):\n",
    "    #     if not any(rel[\"rel\"] in EQUALITY_DEPS and rel[\"to\"] == item_idx for rel in rels):\n",
    "    #         item_tree_idx_list = _extract_isolated_tree(item_idx, rels, toks, without_cc=False, without_strong_deps=False)\n",
    "    #         return [tok for idx, tok in enumerate(toks) if idx in item_tree_idx_list]\n",
    "\n",
    "    #     head_idx = item_idx\n",
    "    #     head_found = False\n",
    "    #     while not head_found:\n",
    "    #         head_found = True\n",
    "    #         for rel in rels:\n",
    "    #             if rel[\"rel\"] in EQUALITY_DEPS and rel[\"to\"] == head_idx:\n",
    "    #                 head_idx = rel[\"from\"]\n",
    "    #                 head_found = False\n",
    "    #                 break\n",
    "    #     head_tree_idx_list = _extract_isolated_tree(head_idx, rels, toks, without_cc=False, without_strong_deps=True)\n",
    "\n",
    "    #     substitute_idx = head_tree_idx_list.index(head_idx)\n",
    "    #     del head_tree_idx_list[substitute_idx]\n",
    "    #     item_tree_idx_list = _extract_isolated_tree(item_idx, rels, toks, without_cc=True, without_strong_deps=False)\n",
    "    #     item_tree_idx_list.reverse()\n",
    "    #     for item_idx in item_tree_idx_list:\n",
    "    #         head_tree_idx_list.insert(substitute_idx, item_idx)\n",
    "\n",
    "    #     return [tok for idx, tok in enumerate(toks) if idx in head_tree_idx_list]\n",
    "        \n",
    "    relation_list, toks, sentence_ranges = _get_all_word_relations(text, ont, morph_an, syntax_an, size_rule)\n",
    "    macro_rel_list = _infer_macro_relations(relation_list, sentence_ranges)\n",
    "\n",
    "    out_sentence_toks = []\n",
    "    for idx in range(len(toks)):\n",
    "        rels = [rel[\"rel\"] for rel in relation_list if rel[\"to\"] == idx]\n",
    "        if any(rel.startswith(\"ont:obj:\") for rel in rels):  # any() returns False on empty input\n",
    "            m_rels = [m_rel for m_rel in macro_rel_list if m_rel[\"to\"] == idx]\n",
    "            sentence = [toks[m_rel[\"from\"]] for m_rel in m_rels if m_rel[\"rel\"] == \"size\"] + \\\n",
    "                [toks[obj_idx] for obj_idx in _extract_isolated_tree(idx, relation_list, toks, only_strong_deps_flag=True)]\n",
    "            out_sentence_toks.append(sentence)\n",
    "\n",
    "    return out_sentence_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd3b7391-86bd-4d0d-a0bf-686a39d26c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['на', 'девочку', 'р', '80-92', 'вещи'],\n",
       " ['на', 'девочку', 'р', '80-92', 'Большая', 'юбка'],\n",
       " ['на', 'девочку', 'р', '80-92', 'зелёные', 'осенние', 'джинсы'],\n",
       " ['на', 'девочку', 'р', '80-92', 'красные', 'кофты']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text_for_rules(\n",
    "    \"Отдам вещи на девочку р 80-92, Большая юбка, зелёные осенние джинсы и красные кофты\", ontology_g, MORPH_AN, SYNTAX_AN, r_size_info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc197705-6ad0-49a6-8b86-b5f2d34ac60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Куртка'], ['Джинсы', 'мужские'], ['Куртка'], ['джинсами']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text_for_rules(\n",
    "    \"Куртка с капюшоном lskdjf. Мужской плащ, Джинсы мужские и женские. Куртка с джинсами.\", ontology_g, MORPH_AN, SYNTAX_AN, r_size_info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dc906ee-2cc4-40e1-8111-1c0d26cb631f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['на', 'девочку', 'р', '80-92', 'вещи'],\n",
       " ['на', 'девочку', 'р', '80-92', 'большая', 'юбка'],\n",
       " ['на', 'девочку', 'р', '80-92', 'зелёные', 'осенние', 'джинсы'],\n",
       " ['на', 'девочку', 'р', '80-92', 'красные', 'кофты']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text_for_rules(\n",
    "    \"Отдам вещи на девочку р 80-92. Большая юбка, зелёные осенние джинсы и красные кофты\", ontology_g, MORPH_AN, SYNTAX_AN, r_size_info\n",
    ")\n",
    "split_text_for_rules(\n",
    "    \"Отдам вещи на девочку р 80-92, большая юбка, зелёные осенние джинсы и красные кофты\", ontology_g, MORPH_AN, SYNTAX_AN, r_size_info\n",
    ")\n",
    "split_text_for_rules(\n",
    "    \"Отдам вещи на девочку р 80-92: большая юбка, зелёные осенние джинсы и красные кофты\", ontology_g, MORPH_AN, SYNTAX_AN, r_size_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28727212-7adb-4ff6-b021-1b7aeaa74c3f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a2165f3-3f5e-406f-90da-ecbc1e852cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert \"ё\" to \"е\", correct typos, correct terms, correct (unify) dashes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "534de60e-4bb7-4431-9897-9c72bd524579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_letter_toks_to_value(size_letters, gender_name, max_x_count):\n",
    "\n",
    "    def lead_number_to_x(size_info, max_x_count):\n",
    "        first_digits = []\n",
    "        letters_started = False\n",
    "        end_letter_reached = False\n",
    "        res = []\n",
    "        for pos, c in enumerate(size_info):\n",
    "            if c.isdigit():\n",
    "                first_digits.append(c)\n",
    "                continue\n",
    "            if len(first_digits) > 0:\n",
    "                digit_val = max(1, min(int(\"\".join(first_digits)), max_x_count))\n",
    "                res = \"\".join([\"x\"] * digit_val)\n",
    "                if c.lower() != \"x\":\n",
    "                    res += size_info[pos:]\n",
    "                else:\n",
    "                    res += size_info[pos + 1:]\n",
    "            else:\n",
    "                res = size_info\n",
    "            break\n",
    "        return res.lower()\n",
    "\n",
    "    def letters_to_range(letters, gender_code):\n",
    "        m_letters_to_size_map = {\n",
    "            \"xs\": (40, 44),\n",
    "            \"s\": (42, 48),\n",
    "            \"m\": (44, 50),\n",
    "            \"l\": (48, 52),\n",
    "            \"xl\": (50, 56),\n",
    "            \"xxl\": (52, 60),\n",
    "            \"xxxl\": (54, 64),\n",
    "            \"xxxxl\": (56, 66),\n",
    "            \"xxxxxl\": (58, 70),\n",
    "            \"xxxxxxl\": (60, 72),\n",
    "            \"xxxxxxxl\": (62, 74),\n",
    "            \"xxxxxxxxl\": (64, 76),\n",
    "            \"xxxxxxxxxl\": (66, 78),\n",
    "            \"xxxxxxxxxxl\": (68, 80),\n",
    "        }\n",
    "        w_letters_to_size_map = {\n",
    "            \"xxxs\": (36, 36),\n",
    "            \"xxs\": (38, 38),\n",
    "            \"xs\": (38, 44),\n",
    "            \"s\": (42, 46),\n",
    "            \"m\": (44, 48),\n",
    "            \"l\": (46, 50),\n",
    "            \"xl\": (48, 54),\n",
    "            \"xxl\": (50, 58),\n",
    "            \"xxxl\": (52, 64),\n",
    "            \"xxxxl\": (54, 66),\n",
    "            \"xxxxxl\": (56, 70),\n",
    "            \"xxxxxxl\": (58, 74),\n",
    "            \"xxxxxxxl\": (56, 78),\n",
    "            \"xxxxxxxxl\": (58, 82),\n",
    "        }\n",
    "\n",
    "        if gender_code == \"m\":\n",
    "            mapper = m_letters_to_size_map\n",
    "        else:\n",
    "            mapper = w_letters_to_size_map\n",
    "\n",
    "        if letters not in mapper:\n",
    "            if letters[-1] == \"l\":\n",
    "                res_range = (max(max(v) for v in mapper.values()), MAX_CLOTHES_SIZE_INT)\n",
    "            else:\n",
    "                res_range = (MIN_CLOTHES_SIZE_INT, min(min(v) for v in mapper.values()))\n",
    "        else:\n",
    "            res_range = mapper[letters]\n",
    "\n",
    "        assert res_range[0] <= res_range[1]\n",
    "        return res_range\n",
    "\n",
    "    size_letters = lead_number_to_x(size_letters, max_x_count)\n",
    "\n",
    "    if gender_name is None:\n",
    "        m_range = letters_to_range(size_letters, \"m\")\n",
    "        w_range = letters_to_range(size_letters, \"w\")\n",
    "        size_range = (min(m_range[0], w_range[0]), max(m_range[1], w_range[1]))\n",
    "    elif gender_name == MAIN_M_GENDER_NAME_STR:\n",
    "        size_range = letters_to_range(size_letters, \"m\")\n",
    "    elif gender_name == MAIN_W_GENDER_NAME_STR:\n",
    "        size_range = letters_to_range(size_letters, \"w\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown gender name: {gender_name}\")\n",
    "\n",
    "    return size_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e08c9a7-8b26-43a6-877c-4f78ea02bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_size_info(orig_fact):\n",
    "\n",
    "    def direct_info_to_range(fact, gender_name):\n",
    "\n",
    "        def _number_toks_to_value(number_info):\n",
    "            if number_info.frac_part is not None:\n",
    "                res = float(f\"{number_info.int_part}.{number_info.frac_part}\")\n",
    "            else:\n",
    "                res = int(number_info.int_part)\n",
    "            return res\n",
    "\n",
    "        size_info = fact.direct_values\n",
    "        info_type = size_info.__class__.__name__\n",
    "        if info_type == \"size_number_list\":\n",
    "            size_from = _number_toks_to_value(size_info.from_info)\n",
    "            if size_info.to_info is None:\n",
    "                size_to = size_from\n",
    "            else:\n",
    "                size_to = _number_toks_to_value(size_info.to_info)\n",
    "            size_range = (size_from, size_to)\n",
    "        elif info_type == \"size_letters_list\":\n",
    "            range_from = size_letter_toks_to_value(size_info.from_info.letters, gender_name, MAX_CLOTHES_SIZE_X_COUNT)\n",
    "            if size_info.to_info is None:\n",
    "                range_to = range_from\n",
    "            else:\n",
    "                range_to = size_letter_toks_to_value(size_info.to_info.letters, gender_name, MAX_CLOTHES_SIZE_X_COUNT)\n",
    "            size_range = (min(range_from), max(range_to))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown info type \\\"{info_type}\\\"\")\n",
    "\n",
    "        return size_range\n",
    "\n",
    "    def indirect_info_to_range(size_info, main_obj):\n",
    "        if size_info.keyword == \"мальчик\":\n",
    "            if hasattr(main_obj.value, \"gender\"):\n",
    "                main_obj.value.gender = MAIN_M_GENDER_NAME_STR\n",
    "            size_range = (MIN_CHILD_CLOTHES_SIZE_INT, MAX_CHILD_CLOTHES_SIZE_INT)\n",
    "        elif size_info.keyword == \"девочка\":\n",
    "            if hasattr(main_obj.value, \"gender\"):\n",
    "                main_obj.value.gender = MAIN_W_GENDER_NAME_STR\n",
    "            size_range = (MIN_CHILD_CLOTHES_SIZE_INT, MAX_CHILD_CLOTHES_SIZE_INT)\n",
    "        elif size_info.keyword == \"мужчина\":\n",
    "            if hasattr(main_obj.value, \"gender\"):\n",
    "                main_obj.value.gender = MAIN_M_GENDER_NAME_STR\n",
    "            size_range = (MAX_CHILD_CLOTHES_SIZE_INT, MAX_CLOTHES_SIZE_INT)\n",
    "        elif size_info.keyword == \"женщина\":\n",
    "            if hasattr(main_obj.value, \"gender\"):\n",
    "                main_obj.value.gender = MAIN_W_GENDER_NAME_STR\n",
    "            size_range = (MAX_CHILD_CLOTHES_SIZE_INT, MAX_CLOTHES_SIZE_INT)\n",
    "        elif size_info.keyword == \"ребёнок\":\n",
    "            size_range = (MIN_CLOTHES_SIZE_INT, MAX_CHILD_CLOTHES_SIZE_INT)\n",
    "        elif size_info.keyword == \"взрослый\":\n",
    "            size_range = (MAX_CHILD_CLOTHES_SIZE_INT, MAX_CLOTHES_SIZE_INT)\n",
    "        elif size_info.keyword == \"школьник\":\n",
    "            # in some cases this word can also be applicable to women\n",
    "            if hasattr(main_obj.value, \"gender\") and fact.main_obj.value.gender is None:\n",
    "                main_obj.value.gender = MAIN_M_GENDER_NAME_STR\n",
    "            size_range = (MIN_M_SCHOOL_CLOTHES_SIZE_INT, MAX_M_SCHOOL_CLOTHES_SIZE_INT)\n",
    "        elif size_info.keyword == \"школьница\":\n",
    "            if hasattr(main_obj.value, \"gender\"):\n",
    "                main_obj.value.gender = MAIN_W_GENDER_NAME_STR\n",
    "            size_range = (MIN_W_SCHOOL_CLOTHES_SIZE_INT, MAX_W_SCHOOL_CLOTHES_SIZE_INT)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown keyword: {fact.size_info.keyword}\")\n",
    "\n",
    "        if size_info.year_info_from_y is not None:\n",
    "            year_to_size_map = {\n",
    "                0: (18, 26),\n",
    "                1: (26, 28),\n",
    "                2: (28, 30),\n",
    "                3: (28, 30),\n",
    "                4: (30, 30),\n",
    "                5: (30, 32),\n",
    "                6: (32, 34),\n",
    "                7: (34, 36),\n",
    "                8: (34, 36),\n",
    "                9: (36, 36),\n",
    "                10: (36, 36),\n",
    "                11: (36, 38),\n",
    "                12: (36, 38),\n",
    "                13: (38, 40),\n",
    "                14: (38, 40),\n",
    "            }\n",
    "            if size_info.year_info_to_y is None:\n",
    "                size_info.year_info_to_y = size_info.year_info_from_y\n",
    "            from_y = int(size_info.year_info_from_y)\n",
    "            to_y = int(size_info.year_info_to_y)\n",
    "\n",
    "            size_from = year_to_size_map.get(from_y, (MAX_CHILD_CLOTHES_SIZE_INT, size_range[1]))\n",
    "            size_to = year_to_size_map.get(to_y, (size_range[0], MAX_CLOTHES_SIZE_INT))\n",
    "            size_range = (min(size_from), max(size_to))\n",
    "        elif size_info.year_info_from_m is not None:\n",
    "            month_to_size_map = {\n",
    "                0: (18, 18),\n",
    "                1: (18, 20),\n",
    "                2: (18, 20),\n",
    "                3: (18, 22),\n",
    "                4: (20, 22),\n",
    "                5: (20, 22),\n",
    "                6: (20, 24),\n",
    "                7: (22, 24),\n",
    "                8: (22, 24),\n",
    "                9: (22, 26),\n",
    "                10: (24, 26),\n",
    "                11: (24, 26),\n",
    "                12: (24, 26),\n",
    "            }\n",
    "            if size_info.year_info_to_m is None:\n",
    "                size_info.year_info_to_m = size_info.year_info_from_m\n",
    "            from_m = int(size_info.year_info_from_m)\n",
    "            to_m = int(size_info.year_info_to_m)\n",
    "\n",
    "            size_from = month_to_size_map.get(from_m, (MAX_CHILD_CLOTHES_SIZE_INT, size_range[1]))\n",
    "            size_to = month_to_size_map.get(to_m, (size_range[0], MAX_CLOTHES_SIZE_INT))\n",
    "            size_range = (min(size_from), max(size_to))\n",
    "        else:\n",
    "            # no info is present\n",
    "            pass\n",
    "\n",
    "        return size_range\n",
    "\n",
    "    if orig_fact.PARSED_size_info is None:\n",
    "        return orig_fact\n",
    "   \n",
    "    obj_class_name = orig_fact.PARSED_size_info.__class__.__name__\n",
    "    if obj_class_name == \"size_info\":\n",
    "        if orig_fact.PARSED_size_info.direct_values is not None:\n",
    "            size_range = direct_info_to_range(orig_fact.PARSED_size_info.direct_values, orig_fact.main_obj.value.gender)\n",
    "        elif orig_fact.PARSED_size_info.indirect_values is not None:\n",
    "            size_range = indirect_info_to_range(orig_fact.PARSED_size_info.indirect_values, orig_fact.main_obj)\n",
    "        else:\n",
    "            raise ValueError(\"Both size infos are None, while object itself is not\")\n",
    "    else:\n",
    "        raise ValueError(f\"No handler for object \\\"{obj_class_name}\\\"\")\n",
    "\n",
    "    if size_range[0] > size_range[1]:\n",
    "        size_range = (size_range[1], size_range[0])\n",
    "\n",
    "    orig_fact.PARSED_size_info = size_range\n",
    "    assert isinstance(orig_fact.PARSED_size_info, tuple) and len(orig_fact.PARSED_size_info) == 2\n",
    "\n",
    "    return orig_fact\n",
    "\n",
    "\n",
    "def get_facts(text, rule_parsers):\n",
    "    sent_variants = [\" \".join(sent_toks) for sent_toks in split_text_for_rules(text, ontology_g, MORPH_AN, SYNTAX_AN, r_size_info)]\n",
    "    trees = []\n",
    "    for parser in rule_parsers:\n",
    "        matched_trees = []\n",
    "        for sentence in sent_variants:\n",
    "            matched_trees += list(parser.findall(sentence))\n",
    "        if len(matched_trees) == 0:\n",
    "            continue\n",
    "        # for each parser we take only longest matches, that aren't overlapped from left to right\n",
    "        matched_trees = sorted(matched_trees, key=lambda m: (m.span.stop - m.span.start, m.span.start), reverse=True)\n",
    "        taken_trees = [matched_trees[0]]\n",
    "        for m_tree in matched_trees[1:]:\n",
    "            if all(m_tree.span.stop <= taken_tree.span.start or m_tree.span.start >= taken_tree.span.stop for taken_tree in taken_trees):\n",
    "                taken_trees.append(m_tree)\n",
    "        trees += taken_trees\n",
    "    return [decode_size_info(tree.fact) for tree in trees]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aad495b-ad4e-4a7e-881f-c6c6047e0e9e",
   "metadata": {},
   "source": [
    "Words are conversted to normal form by parsers, so text preprocessing is not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cd68dc9-718f-498f-8263-1630a72cd078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Jacket_size_proxy(\n",
       "     main_obj=Jacket_attr_vars_proxy(\n",
       "         value=Jacket(\n",
       "             gender=None,\n",
       "             season=None,\n",
       "             material='кожаный'\n",
       "         )\n",
       "     ),\n",
       "     PARSED_size_info=None\n",
       " )]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_facts(\"8. Кожаная куртка р. 40\", rule_parsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "705567dc-e034-4907-876c-b0479fb53750",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ad_facts = [get_facts(text, rule_parsers) for text in ads_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ed4aa64-72c6-498a-bacb-5543e799d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_req_facts = [get_facts(text, rule_parsers) for text in requests_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "737805a6-17d7-4df6-8348-7dd715af3bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coat_size_proxy: 4 advertisements, 32 requests\n",
      "Sweater_size_proxy: 4 advertisements, 0 requests\n",
      "Trousers_size_proxy: 5 advertisements, 1 requests\n",
      "Blouse_size_proxy: 1 advertisements, 0 requests\n",
      "Shirt_size_proxy: 2 advertisements, 1 requests\n",
      "Jacket_size_proxy: 3 advertisements, 23 requests\n",
      "Skirt_size_proxy: 4 advertisements, 0 requests\n"
     ]
    }
   ],
   "source": [
    "fact_counts = {}\n",
    "for ad_facts in all_ad_facts:\n",
    "    for ad_fact in ad_facts:\n",
    "        f_name = ad_fact.__class__.__name__\n",
    "        if f_name not in fact_counts:\n",
    "            fact_counts[f_name] = [0, 0]\n",
    "        fact_counts[f_name][0] += 1\n",
    "for req_facts in all_req_facts:\n",
    "    for req_fact in req_facts:\n",
    "        f_name = req_fact.__class__.__name__\n",
    "        if f_name not in fact_counts:\n",
    "            fact_counts[f_name] = [0, 0]\n",
    "        fact_counts[f_name][1] += 1\n",
    "\n",
    "for fact_name, (ad_cnt, req_cnt) in fact_counts.items():\n",
    "    print(f\"{fact_name}: {ad_cnt} advertisements, {req_cnt} requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a0e7106-9d52-4007-b6e1-8688b4412bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 298 ms, sys: 15.9 ms, total: 314 ms\n",
      "Wall time: 165 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Jacket_size_proxy(\n",
       "     main_obj=Jacket_attr_vars_proxy(\n",
       "         value=Jacket(\n",
       "             gender=None,\n",
       "             season=None,\n",
       "             material='джинсовый'\n",
       "         )\n",
       "     ),\n",
       "     PARSED_size_info=None\n",
       " ),\n",
       " Sweater_size_proxy(\n",
       "     main_obj=Sweater_attr_vars_proxy(\n",
       "         value=Sweater(\n",
       "             gender=None,\n",
       "             season=None,\n",
       "             material=None\n",
       "         )\n",
       "     ),\n",
       "     PARSED_size_info=None\n",
       " )]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_facts(\"джинсовые куртка с кофтой\", rule_parsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b9b4d19-b594-42c4-9b1c-57641578b99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 153 ms, sys: 16 ms, total: 169 ms\n",
      "Wall time: 136 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Jacket_size_proxy(\n",
       "     main_obj=Jacket_attr_vars_proxy(\n",
       "         value=Jacket(\n",
       "             gender=None,\n",
       "             season=None,\n",
       "             material=None\n",
       "         )\n",
       "     ),\n",
       "     PARSED_size_info=None\n",
       " )]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_facts(\"куртка из кожи\", rule_parsers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcec9e2-fa95-49e3-94c1-ac70bf0fb979",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0701cd74-2739-454c-b72a-b2c42281ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_facts_close(req_facts, ad_facts):\n",
    "    for req_fact in req_facts:\n",
    "        for ad_fact in ad_facts:\n",
    "            if req_fact.__class__.__name__ != ad_fact.__class__.__name__:\n",
    "                continue\n",
    "            is_match = True\n",
    "            for attr_name in req_fact.__attributes__:\n",
    "                ad_attr = getattr(ad_fact, attr_name)\n",
    "                req_attr = getattr(req_fact, attr_name)\n",
    "                if req_attr is not None and ad_attr is not None:\n",
    "                    # different attributes are not match, but if this attribute is omitted in request or ad, this is still match\n",
    "                    if attr_name == \"PARSED_size_info\":\n",
    "                        if max(req_attr) < min(ad_attr) or min(req_attr) > max(ad_attr):\n",
    "                            # any intersection of sized is a match, but no intersection means no metch\n",
    "                            is_match = False\n",
    "                            break\n",
    "                    elif req_attr != ad_attr:\n",
    "                        is_match = False\n",
    "                        break\n",
    "            if not is_match:\n",
    "                continue\n",
    "            # even one matched fact is complete match between request and ad\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def predict_by_facts(req_fact_list, ad_fact_list):\n",
    "    predictions = {}\n",
    "    for req_id, req_facts in enumerate(req_fact_list, start=1):\n",
    "        found_list = []\n",
    "        for ad_id, ad_facts in enumerate(ad_fact_list, start=1):\n",
    "            if are_facts_close(req_facts, ad_facts):\n",
    "                found_list.append(str(ad_id))\n",
    "        if len(found_list) > 0:\n",
    "            predictions[str(req_id)] = found_list.copy()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "665f8125-9152-4406-9c25-884c7b619632",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_markup = predict_by_facts(all_req_facts, all_ad_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70d88008-02ea-4689-98c6-b2167c8450bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 89, 'FP': 27, 'TN': 87198, 'FN': 508}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = metrics.calc_confusion_matrix(true_markup, pred_markup, n_ads=len(ads_raw), n_requests=len(requests_raw))\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46f9a6a7-79ca-4e5c-bf5d-9c472ebd9142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"False positives:\")\n",
    "# for req_id, matched_ad_ids in pred_markup.items():\n",
    "#     found_fp_ids = []\n",
    "#     for ad_id in matched_ad_ids:\n",
    "#         if req_id not in true_markup or ad_id not in true_markup[req_id]:\n",
    "#             found_fp_ids.append(ad_id)\n",
    "#     if len(found_fp_ids) > 0:\n",
    "#         print(f\"\\t{req_id}. \\\"{requests_raw[int(req_id) - 1].strip()}\\\" => {all_req_facts[int(req_id) - 1]}\")\n",
    "#     for ad_id in found_fp_ids:\n",
    "#         print(f\"\\t\\t{ad_id}) {ads_raw[int(ad_id) - 1].strip()} => {all_ad_facts[int(ad_id) - 1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3aaa536-a1ee-41dd-8fa8-7d30d1f44af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9939081323586345,\n",
       " 'precision': 0.7672413793103449,\n",
       " 'recall': 0.1490787269681742,\n",
       " 'f1': 0.2496493688639551}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = metrics.calc_all_stats(confusion_matrix)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83872ea7-2336-44db-a648-ab10741196fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "|\tMetric\t\t|\tOld Value\t|\tNew Value\t|\tDiff\t|\n",
      "-----------------------------------------------------------------------------------------\n",
      "|\tTP\t\t|\t216\t\t|\t89\t\t|\t📉 -127\t|\n",
      "|\tFP\t\t|\t418\t\t|\t27\t\t|\t📉 -391\t|\n",
      "|\tTN\t\t|\t86810\t\t|\t87198\t\t|\t📈 388\t|\n",
      "|\tFN\t\t|\t378\t\t|\t508\t\t|\t📈 130\t|\n",
      "|\tPrec\t\t|\t0.341\t\t|\t0.767\t\t|\t📈 0.427\t|\n",
      "|\tRecall\t\t|\t0.364\t\t|\t0.149\t\t|\t📉 -0.215\t|\n",
      "|\tF1\t\t|\t0.352\t\t|\t0.250\t\t|\t📉 -0.102\t|\n",
      "\n",
      "F1 📉 decreased by 0.102, down to 25.0%, which is a significant fall.\n"
     ]
    }
   ],
   "source": [
    "metrics.compare_with_saved_stats(stats, confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a86a0c-8e79-4235-a4de-817e8fdf9eb4",
   "metadata": {},
   "source": [
    "## Topics for Learning Yargy\n",
    "\n",
    "Documentation:\n",
    "* https://nbviewer.org/github/natasha/yargy/blob/master/docs/index.ipynb\n",
    "* https://nbviewer.org/github/natasha/yargy/blob/master/docs/ref.ipynb\n",
    "* https://nbviewer.org/github/natasha/yargy/blob/master/docs/cookbook.ipynb\n",
    "\n",
    "Topics for paying attention to:\n",
    "1. Main terms and entities: rule, fact (+interpretation stage), predicate, gazetteer\n",
    "1. Multiple values for single attribute are not supported\n",
    "1. Rules for arbitrary order of words (\"adjacency\") are not supported, so they are generated\n",
    "1. Hierarchical relationship of objects in rules looks not supported (i.e. input to rules are bare words, not objects), but it needs to be checked\n",
    "1. We can match word not only literally or by normal form, but also by POS, regex, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903ddd3-5500-4074-b71d-63cc475897ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
