{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881a427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b164cd-d98b-48d2-9089-4b3fc1bdf1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_SETTINGS = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ce300-80ff-4a83-80bf-3a94df754cb4",
   "metadata": {},
   "source": [
    "# Calculate Metrics for CLIP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745569d1",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1da0f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_markup(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if len(line) == 0:\n",
    "                break\n",
    "            sep_ind = line.find(\"\\\"\")\n",
    "            path = (\"../\" + line[:sep_ind]).strip()\n",
    "            desc = line[sep_ind + 1:].strip()[:-1]\n",
    "            data.append((path, desc))\n",
    "    return data\n",
    "\n",
    "\n",
    "img_markup = load_image_markup(\"../data/matching_images.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26effb",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39ea826c-7e2b-4039-9782-a63c3a62c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(model_name, markup, threshold):\n",
    "    model, preprocess_function = clip.load(model_name, device=DEVICE_SETTINGS)\n",
    "\n",
    "    text_list = [markup_line[1].lower() for markup_line in markup]\n",
    "    # some descriptions are the same or mostly same (start from the same words)\n",
    "    unique_indices = []\n",
    "    for idx in range(len(text_list)):\n",
    "        if idx == 0:\n",
    "            unique_indices.append(idx)\n",
    "            continue\n",
    "        prev_started = any(txt.startswith(text_list[idx]) for txt in text_list[:idx])\n",
    "        current_started = any(text_list[idx].startswith(txt) for txt in text_list[:idx])\n",
    "        if prev_started or current_started:\n",
    "            continue\n",
    "        unique_indices.append(idx)\n",
    "    unique_text_list = [text_list[idx] for idx in unique_indices]\n",
    "    unique_text_list_pt = clip.tokenize(unique_text_list).to(DEVICE_SETTINGS)\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for img_idx, markup_line in tqdm(enumerate(markup), total=len(text_list)):\n",
    "        with torch.no_grad():\n",
    "            image_pt = preprocess_function(PIL.Image.open(markup_line[0])).unsqueeze(0).to(DEVICE_SETTINGS)\n",
    "\n",
    "            logits_per_image_pt, logits_per_text_pt = model(image_pt, unique_text_list_pt)\n",
    "            # probs = logits_per_image_pt.softmax(dim=-1).cpu().numpy()  # can't be used for not fixed list of variants\n",
    "            logits_per_image = logits_per_image_pt[0].cpu().numpy()\n",
    "            # logits_per_image = 1/(1 + np.exp(-logits_per_image))  # does not work with enough accuracy: all probs are too close to 1\n",
    "            # print(f\"dbg: {logits_per_image = }\")  # use this debug printout to setup threshold manualy\n",
    "\n",
    "        if img_idx not in unique_indices:\n",
    "            assert markup_line[1].lower() == text_list[img_idx]\n",
    "            txt_to_find = markup_line[1].lower()\n",
    "            u_img_idx = -1\n",
    "            for i in range(len(unique_text_list)):\n",
    "                if unique_text_list[i].startswith(txt_to_find) or txt_to_find.startswith(unique_text_list[i]):\n",
    "                    u_img_idx = i\n",
    "                    break\n",
    "            assert u_img_idx >= 0\n",
    "            assert u_img_idx < img_idx\n",
    "        else:\n",
    "            u_img_idx = unique_indices.index(img_idx)\n",
    "        found_indices = np.argwhere(logits_per_image >= threshold)[:, 0].tolist()\n",
    "\n",
    "        if u_img_idx in found_indices:\n",
    "            tp += 1\n",
    "            if len(found_indices) > 1:\n",
    "                fp += len(found_indices) - 1\n",
    "            tn += len(unique_indices) - len(found_indices)\n",
    "        else:\n",
    "            fn += 1\n",
    "            fp += len(found_indices)\n",
    "            tn += len(unique_indices) - len(found_indices) - 1\n",
    "\n",
    "    assert tp + fp + tn + fn == len(unique_text_list)*len(text_list)\n",
    "\n",
    "    return tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "021a6aed-aa05-4485-9e15-1fbd75d62054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ffdbb7b-87d0-4855-82fb-977823e73f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_THRS = {  # these thresholds were picked up manually after brief review of logits, so they are not accurate\n",
    "    \"RN50\": 20,\n",
    "    \"RN101\": 46,\n",
    "    # \"RN50x4\": 36,   # eats too much resources, skipped for full measurements\n",
    "    # \"RN50x16\": 27,  # eats too much resources, skipped for full measurements\n",
    "    # \"RN50x64\": 17,  # eats too much resources, skipped for full measurements\n",
    "    # \"ViT-B/32\": 29,  # eats too much resources, skipped for full measurements\n",
    "    # \"ViT-B/16\": 28,  # eats too much resources, skipped for full measurements\n",
    "    \"ViT-L/14\": 23,\n",
    "    # \"ViT-L/14@336px\": 23,  # eats too much resources, skipped for full measurements\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca24e018-6744-4509-86d2-3e9733fc886d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 311/311 [43:09<00:00,  8.33s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 311/311 [42:38<00:00,  8.23s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 311/311 [1:23:31<00:00, 16.11s/it]\n"
     ]
    }
   ],
   "source": [
    "metric_dict = {\"Model\": [], \"TP\": [], \"FP\": [], \"TN\": [], \"FN\": [], \"Accuracy\": [], \"F1\": []}\n",
    "for model_name, model_thr in MODEL_THRS.items():\n",
    "    tp, fp, tn, fn = calc_metrics(model_name, img_markup, model_thr)\n",
    "\n",
    "    if 2*tp + fp + fn > 0:\n",
    "        f1 = 2*tp/(2*tp + fp + fn)\n",
    "    else:\n",
    "        f1 = 0\n",
    "    \n",
    "    if tp + fp + tn + fn > 0:\n",
    "        acc = (tp + tn)/(tp + fp + tn + fn)\n",
    "    else:\n",
    "        acc = 0\n",
    "\n",
    "    metric_dict[\"Model\"].append(model_name)\n",
    "    metric_dict[\"TP\"].append(tp)\n",
    "    metric_dict[\"FP\"].append(fp)\n",
    "    metric_dict[\"TN\"].append(tn)\n",
    "    metric_dict[\"FN\"].append(fn)\n",
    "    metric_dict[\"Accuracy\"].append(acc)\n",
    "    metric_dict[\"F1\"].append(f1)\n",
    "\n",
    "metric_df = pd.DataFrame(metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b35f56-4175-4271-ba33-079437c820d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RN50</td>\n",
       "      <td>262</td>\n",
       "      <td>3618</td>\n",
       "      <td>72577</td>\n",
       "      <td>49</td>\n",
       "      <td>0.952069</td>\n",
       "      <td>0.125030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RN101</td>\n",
       "      <td>194</td>\n",
       "      <td>747</td>\n",
       "      <td>75448</td>\n",
       "      <td>117</td>\n",
       "      <td>0.988707</td>\n",
       "      <td>0.309904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ViT-L/14</td>\n",
       "      <td>259</td>\n",
       "      <td>1395</td>\n",
       "      <td>74800</td>\n",
       "      <td>52</td>\n",
       "      <td>0.981086</td>\n",
       "      <td>0.263613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model   TP    FP     TN   FN  Accuracy        F1\n",
       "0      RN50  262  3618  72577   49  0.952069  0.125030\n",
       "1     RN101  194   747  75448  117  0.988707  0.309904\n",
       "2  ViT-L/14  259  1395  74800   52  0.981086  0.263613"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d93ee-846b-41c2-8c35-36a084101565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
