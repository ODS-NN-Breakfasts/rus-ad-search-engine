{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881a427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b164cd-d98b-48d2-9089-4b3fc1bdf1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_SETTINGS = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ce300-80ff-4a83-80bf-3a94df754cb4",
   "metadata": {},
   "source": [
    "# Calculate Metrics for CLIP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745569d1",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1da0f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_markup(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if len(line) == 0:\n",
    "                break\n",
    "            sep_ind = line.find(\"\\\"\")\n",
    "            path = (\"../\" + line[:sep_ind]).strip()\n",
    "            desc = line[sep_ind + 1:].strip()[:-1]\n",
    "            data.append((path, desc))\n",
    "    return data\n",
    "\n",
    "\n",
    "img_markup = load_image_markup(\"../data/matching_images.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26effb",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ea826c-7e2b-4039-9782-a63c3a62c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(model_name, markup):\n",
    "    model, preprocess_function = clip.load(model_name, device=DEVICE_SETTINGS)\n",
    "\n",
    "    text_list = [markup_line[1].lower() for markup_line in markup]\n",
    "    # some descriptions are the same or mostly same (start from the same words)\n",
    "    unique_indices = []\n",
    "    for idx in range(len(text_list)):\n",
    "        if idx == 0:\n",
    "            unique_indices.append(idx)\n",
    "            continue\n",
    "        prev_started = any(txt.startswith(text_list[idx]) for txt in text_list[:idx])\n",
    "        current_started = any(text_list[idx].startswith(txt) for txt in text_list[:idx])\n",
    "        if prev_started or current_started:\n",
    "            continue\n",
    "        unique_indices.append(idx)\n",
    "    unique_text_list = [text_list[idx] for idx in unique_indices]\n",
    "    unique_text_list_pt = clip.tokenize(unique_text_list).to(DEVICE_SETTINGS)\n",
    "\n",
    "    y_true_list = []\n",
    "    logit_list = []\n",
    "    for img_idx, markup_line in tqdm(enumerate(markup), total=len(text_list)):\n",
    "        with torch.no_grad():\n",
    "            image_pt = preprocess_function(PIL.Image.open(markup_line[0])).unsqueeze(0).to(DEVICE_SETTINGS)\n",
    "\n",
    "            logits_per_image_pt, logits_per_text_pt = model(image_pt, unique_text_list_pt)\n",
    "            # probs = logits_per_image_pt.softmax(dim=-1).cpu().numpy()  # can't be used for not fixed list of variants\n",
    "            logits_per_image = logits_per_image_pt[0].cpu().numpy()\n",
    "            # logits_per_image = 1/(1 + np.exp(-logits_per_image))  # does not work with enough accuracy: all probs are too close to 1\n",
    "\n",
    "        if img_idx not in unique_indices:\n",
    "            assert markup_line[1].lower() == text_list[img_idx]\n",
    "            txt_to_find = markup_line[1].lower()\n",
    "            u_img_idx = -1\n",
    "            for i in range(len(unique_text_list)):\n",
    "                if unique_text_list[i].startswith(txt_to_find) or txt_to_find.startswith(unique_text_list[i]):\n",
    "                    u_img_idx = i\n",
    "                    break\n",
    "            assert u_img_idx >= 0\n",
    "            assert u_img_idx < img_idx\n",
    "        else:\n",
    "            u_img_idx = unique_indices.index(img_idx)\n",
    "\n",
    "        y_true_list += [1 if idx == u_img_idx else 0 for idx in range(len(unique_text_list))]\n",
    "        logit_list += logits_per_image.tolist()\n",
    "\n",
    "    y_true_vec = np.array(y_true_list)\n",
    "    logit_vec = np.array(logit_list)\n",
    "    assert y_true_vec.shape == logit_vec.shape\n",
    "\n",
    "    l_min = logit_vec.min()\n",
    "    l_max = logit_vec.max()\n",
    "    logit_vec = (logit_vec - l_min)/(l_max - l_min)\n",
    "\n",
    "    # https://stats.stackexchange.com/q/287117/\n",
    "    prevalence = np.count_nonzero(y_true_vec == 1, keepdims=False)/len(y_true_vec)\n",
    "    assert prevalence > 0\n",
    "    fpr_vec, tpr_vec, thr_vec = sklearn.metrics.roc_curve(y_true_vec, logit_vec)\n",
    "    recall_vec = tpr_vec\n",
    "    tnr_vec = 1 - fpr_vec\n",
    "\n",
    "    zero_div_idxs = np.where((recall_vec*prevalence) + ((1 - tnr_vec)*(1 - prevalence)) == 0)[0]\n",
    "    if zero_div_idxs.size > 0:\n",
    "        # to avoid zero-division warning from numpy below\n",
    "        recall_vec[zero_div_idxs] += 1e-8\n",
    "    precision_vec = (recall_vec*prevalence)/((recall_vec*prevalence) + ((1 - tnr_vec)*(1 - prevalence)))\n",
    "\n",
    "    zero_div_idxs = np.where(precision_vec + recall_vec == 0)[0]\n",
    "    if zero_div_idxs.size > 0:\n",
    "        # to avoid zero-division warning from numpy below\n",
    "        precision_vec[zero_div_idxs] += 1e-8\n",
    "        recall_vec[zero_div_idxs] += 1e-8\n",
    "    f1_vec = 2*(precision_vec*recall_vec)/(precision_vec + recall_vec)\n",
    "\n",
    "    opt_thr = thr_vec[np.argmax(f1_vec)]\n",
    "    if np.isinf(opt_thr):\n",
    "        opt_thr = 1\n",
    "\n",
    "    # using \">=\" below instead of \">\" is extremely important, because sklearn cn return edge values for threshold\n",
    "    tp = np.count_nonzero((logit_vec >= opt_thr) & (y_true_vec == 1))\n",
    "    fp = np.count_nonzero((logit_vec >= opt_thr) & (y_true_vec == 0))\n",
    "    tn = np.count_nonzero((logit_vec < opt_thr) & (y_true_vec == 0))\n",
    "    fn = np.count_nonzero((logit_vec < opt_thr) & (y_true_vec == 1))\n",
    "\n",
    "    opt_thr = float(opt_thr)*(l_max - l_min) + l_min\n",
    "\n",
    "    return tp, fp, tn, fn, opt_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "021a6aed-aa05-4485-9e15-1fbd75d62054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ffdbb7b-87d0-4855-82fb-977823e73f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_THRS = {  # these thresholds were picked up manually after brief review of logits, so they are not accurate\n",
    "    \"RN50\": 20,\n",
    "    \"RN101\": 46,\n",
    "    # \"RN50x4\": 36,   # eats too much resources, skipped for full measurements\n",
    "    # \"RN50x16\": 27,  # eats too much resources, skipped for full measurements\n",
    "    # \"RN50x64\": 17,  # eats too much resources, skipped for full measurements\n",
    "    # \"ViT-B/32\": 29,  # eats too much resources, skipped for full measurements\n",
    "    # \"ViT-B/16\": 28,  # eats too much resources, skipped for full measurements\n",
    "    \"ViT-L/14\": 23,\n",
    "    # \"ViT-L/14@336px\": 23,  # eats too much resources, skipped for full measurements\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca24e018-6744-4509-86d2-3e9733fc886d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "metric_dict = {\"Model\": [], \"TP\": [], \"FP\": [], \"TN\": [], \"FN\": [], \"Threshold\": [], \"Accuracy\": [], \"F1\": []}\n",
    "for model_name, model_thr in MODEL_THRS.items():\n",
    "    tp, fp, tn, fn, thr = calc_metrics(model_name, img_markup)\n",
    "\n",
    "    if 2*tp + fp + fn > 0:\n",
    "        f1 = 2*tp/(2*tp + fp + fn)\n",
    "    else:\n",
    "        f1 = 0\n",
    "    \n",
    "    if tp + fp + tn + fn > 0:\n",
    "        acc = (tp + tn)/(tp + fp + tn + fn)\n",
    "    else:\n",
    "        acc = 0\n",
    "\n",
    "    metric_dict[\"Model\"].append(model_name)\n",
    "    metric_dict[\"TP\"].append(tp)\n",
    "    metric_dict[\"FP\"].append(fp)\n",
    "    metric_dict[\"TN\"].append(tn)\n",
    "    metric_dict[\"FN\"].append(fn)\n",
    "    metric_dict[\"Threshold\"].append(thr)\n",
    "    metric_dict[\"Accuracy\"].append(acc)\n",
    "    metric_dict[\"F1\"].append(f1)\n",
    "\n",
    "metric_df = pd.DataFrame(metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b35f56-4175-4271-ba33-079437c820d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d93ee-846b-41c2-8c35-36a084101565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
