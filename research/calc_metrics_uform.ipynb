{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639214f6-c836-415c-9062-e33b3f05d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc32849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # works only with old version of transformers\n",
    "# !pip install -U transformers==4.37.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7d2b8f-eb4b-4147-a3fe-c59ac1f93b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # there is a bug in ipykernel that prevents downloading of BAAI/bge-large-en-v1.5, so we need to be sure that it is updated\n",
    "# # https://github.com/huggingface/xet-core/issues/526\n",
    "# !pip install -U ipykernel>=7.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f2e38-d71b-457e-b915-6d283ecc1a27",
   "metadata": {},
   "source": [
    "# Calculate Metrics for UForm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec97dd7e-a891-47b2-934f-10f6e969625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_txt_emb(text):\n",
    "    METRICS_TXT_EMB_MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(METRICS_TXT_EMB_MODEL_NAME)\n",
    "    model = transformers.AutoModel.from_pretrained(METRICS_TXT_EMB_MODEL_NAME)\n",
    "    model.eval()\n",
    "    \n",
    "    encoded_input = tokenizer([text], padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        sentence_embeddings = model_output[0][:, 0]\n",
    "\n",
    "    return torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d2eafe-0f33-4a42-8b84-4e89cb89e0f0",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9e1ee8-259a-4077-bf32-a968e48f3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_markup(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if len(line) == 0:\n",
    "                break\n",
    "            sep_ind = line.find(\"\\\"\")\n",
    "            path = (\"../\" + line[:sep_ind]).strip()\n",
    "            desc = line[sep_ind + 1:].strip()[:-1]\n",
    "            data.append((path, desc))\n",
    "    return data\n",
    "\n",
    "\n",
    "img_markup = load_image_markup(\"../data/matching_images.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956778f6-f7a9-42d9-9f29-499e8d526d6a",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05ad414d-326f-4322-87c3-8ada8c52266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(markup, model, processor_obj):\n",
    "    PROMPT = \"List the objects in the photo in one sentence, no information about background needed\"\n",
    "\n",
    "    text_list = [markup_line[1].lower() for markup_line in markup]\n",
    "    # some descriptions are the same or mostly same (start from the same words)\n",
    "    unique_indices = []\n",
    "    for idx in range(len(text_list)):\n",
    "        if idx == 0:\n",
    "            unique_indices.append(idx)\n",
    "            continue\n",
    "        prev_started = any(txt.startswith(text_list[idx]) for txt in text_list[:idx])\n",
    "        current_started = any(text_list[idx].startswith(txt) for txt in text_list[:idx])\n",
    "        if prev_started or current_started:\n",
    "            continue\n",
    "        unique_indices.append(idx)\n",
    "    unique_text_list = [text_list[idx] for idx in unique_indices]\n",
    "    emb_unique_text_list = [get_metrics_txt_emb(txt) for txt in unique_text_list]\n",
    "\n",
    "    y_true_list = []\n",
    "    distance_list = []\n",
    "    for img_idx, markup_line in tqdm(enumerate(markup), total=len(text_list)):\n",
    "        inputs = processor_obj(text=[PROMPT], images=[PIL.Image.open(markup_line[0])], return_tensors=\"pt\")\n",
    "        with torch.inference_mode():\n",
    "             output = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=False,\n",
    "                use_cache=True,\n",
    "                max_new_tokens=256,\n",
    "                eos_token_id=151645,\n",
    "                pad_token_id=processor_obj.tokenizer.pad_token_id\n",
    "                \n",
    "            )\n",
    "\n",
    "        prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "        decoded_out = processor_obj.batch_decode(output[:, prompt_len:], skip_special_tokens=True)[0]\n",
    "\n",
    "        emb_decoded_out = get_metrics_txt_emb(decoded_out)\n",
    "        distance_per_image = [np.linalg.norm(emb_unique_text - emb_decoded_out) for emb_unique_text in emb_unique_text_list]\n",
    "\n",
    "        if img_idx not in unique_indices:\n",
    "            assert markup_line[1].lower() == text_list[img_idx]\n",
    "            txt_to_find = markup_line[1].lower()\n",
    "            u_img_idx = -1\n",
    "            for i in range(len(unique_text_list)):\n",
    "                if unique_text_list[i].startswith(txt_to_find) or txt_to_find.startswith(unique_text_list[i]):\n",
    "                    u_img_idx = i\n",
    "                    break\n",
    "            assert u_img_idx >= 0\n",
    "            assert u_img_idx < img_idx\n",
    "        else:\n",
    "            u_img_idx = unique_indices.index(img_idx)\n",
    "\n",
    "        y_true_list += [1 if idx == u_img_idx else 0 for idx in range(len(unique_text_list))]\n",
    "        distance_list += distance_per_image\n",
    "\n",
    "    y_true_vec = np.array(y_true_list)\n",
    "    max_distance = max(distance_list)\n",
    "    logit_vec = np.array([(max_distance - dist)/max_distance for dist in distance_list])\n",
    "    assert y_true_vec.shape == logit_vec.shape\n",
    "\n",
    "    assert logit_vec.min() >= 0\n",
    "    assert logit_vec.max() <= 1\n",
    "    assert np.allclose(np.sort(np.unique(y_true_vec)), np.array([0, 1]))\n",
    "\n",
    "    # https://stats.stackexchange.com/q/287117/\n",
    "    prevalence = np.count_nonzero(y_true_vec == 1, keepdims=False)/len(y_true_vec)\n",
    "    assert prevalence > 0\n",
    "    fpr_vec, tpr_vec, thr_vec = sklearn.metrics.roc_curve(y_true_vec, logit_vec)\n",
    "    recall_vec = tpr_vec\n",
    "    tnr_vec = 1 - fpr_vec\n",
    "\n",
    "    zero_div_idxs = np.where((recall_vec*prevalence) + ((1 - tnr_vec)*(1 - prevalence)) == 0)[0]\n",
    "    if zero_div_idxs.size > 0:\n",
    "        # to avoid zero-division warning from numpy below\n",
    "        recall_vec[zero_div_idxs] += 1e-8\n",
    "    precision_vec = (recall_vec*prevalence)/((recall_vec*prevalence) + ((1 - tnr_vec)*(1 - prevalence)))\n",
    "\n",
    "    zero_div_idxs = np.where(precision_vec + recall_vec == 0)[0]\n",
    "    if zero_div_idxs.size > 0:\n",
    "        # to avoid zero-division warning from numpy below\n",
    "        precision_vec[zero_div_idxs] += 1e-8\n",
    "        recall_vec[zero_div_idxs] += 1e-8\n",
    "    f1_vec = 2*(precision_vec*recall_vec)/(precision_vec + recall_vec)\n",
    "\n",
    "    opt_thr = thr_vec[np.argmax(f1_vec)]\n",
    "    if np.isinf(opt_thr):\n",
    "        opt_thr = 1\n",
    "\n",
    "    # using \">=\" below instead of \">\" is extremely important, because sklearn cn return edge values for threshold\n",
    "    tp = np.count_nonzero((logit_vec >= opt_thr) & (y_true_vec == 1))\n",
    "    fp = np.count_nonzero((logit_vec >= opt_thr) & (y_true_vec == 0))\n",
    "    tn = np.count_nonzero((logit_vec < opt_thr) & (y_true_vec == 0))\n",
    "    fn = np.count_nonzero((logit_vec < opt_thr) & (y_true_vec == 1))\n",
    "\n",
    "    tp_list = [\n",
    "        (distance_list[tp_idx], text_list[tp_idx % len(unique_text_list)])\n",
    "        for tp_idx in np.nonzero((logit_vec >= opt_thr) & (y_true_vec == 1))[0]\n",
    "    ]\n",
    "    tp_list = [(sum(dist for dist, txt in tp_list if txt == u_txt), u_txt) for u_txt in set([txt for dist, txt in tp_list])]\n",
    "    tp_list.sort(key=lambda item: item[0])\n",
    "    fp_list = [\n",
    "        (distance_list[fp_idx], text_list[fp_idx % len(unique_text_list)])\n",
    "        for fp_idx in np.nonzero((logit_vec >= opt_thr) & (y_true_vec == 0))[0]\n",
    "    ]\n",
    "    fp_list = [(sum(dist for dist, txt in fp_list if txt == u_txt), u_txt) for u_txt in set([txt for dist, txt in fp_list])]\n",
    "    fp_list.sort(key=lambda item: item[0])\n",
    "    opt_thr = max_distance - (float(opt_thr)*max_distance)\n",
    "\n",
    "    return tp, fp, tn, fn, opt_thr, tp_list, fp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe4cc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cc196698bf419880ecfa7bf413925d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"unum-cloud/uform-gen2-qwen-500m\"\n",
    "model = transformers.AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "processor_func = transformers.AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56614a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 311/311 [55:57<00:00, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE POSITIVES:\n",
      "\t(0.6) black sleeveless dress for girls\n",
      "\t(0.6) women's warm down jacket with zip and hood\n",
      "\t(0.6) dark blue women's dress\n",
      "\t(0.7) women's high black suede boots, women's high beige suede boots\n",
      "\t(0.7) warm zipped jacket with a hood\n",
      "\t(0.7) crib bumpers with bunnies\n",
      "\t(0.7) cover for baby's changing table\n",
      "\t(0.7) 3 doors wooden cabinet, armchair, wooden chairs\n",
      "\t(0.7) black kitten sits on the wooden boards\n",
      "\t(0.7) party dress for a girl\n",
      "\t(0.7) individually wrapped tea bags\n",
      "\t(0.7) abstract patterned floor rug\n",
      "\t(0.7) women's warm down jacket\n",
      "\t(0.7) pink rubber boots for girls with unicorns, black patent leather shoes for girls, pink summer sandals for girls, burgundy summer shoes\n",
      "\t(0.7) inflatable swimming ring clasp\n",
      "\t(0.7) grey kitten sits on the wooden boards\n",
      "\t(0.7) gray windbreaker with a zipper\n",
      "\t(0.7) girls clothing set\n",
      "\t(0.8) feeding baby chair\n",
      "\t(0.8) women's leather low boots with buckles view from the top\n",
      "\t(0.8) glassware, shot glasses, goblets, wine glasses, plates and a metal kitchen spatula\n",
      "\t(0.8) black trousers and a black vest\n",
      "\t(0.8) set of black and grey pants\n",
      "\t(0.8) leather women's boots soles view\n",
      "\t(0.8) school backpack for a girl with an owl\n",
      "\t(0.8) red and yellow plastic children's chairs\n",
      "\t(0.8) baby clothes, blue jumpsuit, striped sleepsuits, knitted red warm jumpsuit\n",
      "\t(0.8) women's fur coat\n",
      "\t(0.8) a bag of children's toys, a bag of children's safety helmets, a children's magnetic construction set\n",
      "\t(0.8) textbooks for schoolchildren to prepare for exams\n",
      "\t(0.8) set of girl's dresses blck, brown and light blue\n",
      "\t(0.8) women's demi-season coat with buttons\n",
      "\t(0.8) large pregnancy pillow\n",
      "\t(0.8) women's beige demi-season coat, women's denim jacket\n",
      "\t(0.8) jewelry, pearl earrings\n",
      "\t(0.8) gray knitted women's dress\n",
      "\t(0.8) dark gray rabbit\n",
      "\t(0.8) individually wrapped candies\n",
      "\t(0.8) a set of glassware, shot glasses, goblets, a porcelain teapot and sugar bowls, an iron, small flower vase\n",
      "\t(0.8) long green women's dress\n",
      "\t(0.8) girls' party dress with black appliques\n",
      "\t(0.8) brown plastic baseboards\n",
      "\t(0.8) a turquoise women's suit consisting of a blouse and a skirt, purple women's dress, black women's dress\n",
      "\t(0.8) baby diapers\n",
      "\t(0.8) blue jeans\n",
      "\t(0.8) porcelain teapots, white with floral print and black, sugar bowls, tea cups and saucers\n",
      "\t(0.8) wheelchair with a black leather seat\n",
      "\t(0.8) sports sneakers\n",
      "\t(0.8) corner cabinet section with shelves, brown wood\n",
      "\t(0.8) metal cage for birds\n",
      "\t(0.8) kitchen plates, plastic bowls\n",
      "\t(0.8) navy women's shirt, white t-shirt, black dress\n",
      "\t(0.8) women's white winter boots with fur trim\n",
      "\t(0.8) dark gray rabbit in a cage\n",
      "\t(0.8) toy cars\n",
      "\t(0.8) green summer women's dress with short sleeves\n",
      "\t(0.8) black summer dress with floral print straps\n",
      "\t(1.4) plant in a pot\n",
      "\t(1.4) toy water gun\n",
      "\t(1.4) warm girls' zippered hooded jacket with a floral print\n",
      "\t(1.5) red and blue boxing gloves\n",
      "\t(2.3) women's denim dress with long sleeves\n",
      "\t(3.5) beige dog\n",
      "\t(3.7) black demi-season ankle boots\n",
      "\n",
      "\n",
      "FALSE POSITIVES:\n",
      "\t(0.7) warm girls' zippered hooded jacket with a floral print\n",
      "\t(0.7) women's warm down jacket with zip and hood back view\n",
      "\t(0.7) inflatable swimming ring\n",
      "\t(0.7) geometric patterned floor rug\n",
      "\t(0.7) individually wrapped tea bags\n",
      "\t(0.8) green baby changing table\n",
      "\t(0.8) women's black leather shoes without heels, women's red shoes with heels\n",
      "\t(0.8) black backpack with white print\n",
      "\t(0.8) women's high black suede boots, women's high beige suede boots\n",
      "\t(0.8) brown beige women's shoes\n",
      "\t(0.8) baby fish\n",
      "\t(0.8) women's black leather high heel shoes with a strap\n",
      "\t(0.8) knitted jumper\n",
      "\t(0.8) children's black leather winter low boots\n",
      "\t(0.8) women's grey dress\n",
      "\t(0.8) bags of cement plaster, paint rollers, plastic buckets\n",
      "\t(0.8) dark blue dress for girls with a star print\n",
      "\t(0.8) dark blue women's dress\n",
      "\t(0.8) blue women's down jacket\n",
      "\t(0.8) a bag of children's toys, a children's helicopter, a children's book, an agry birds figurine\n",
      "\t(0.8) crib with sides, children's mattress\n",
      "\t(0.8) men's ties lies on the floor\n",
      "\t(0.8) black summer dress with floral print straps\n",
      "\t(0.8) school backpack for a girl with an owl\n",
      "\t(0.8) black rectangular leather women's bag\n",
      "\t(0.8) glass for hood\n",
      "\t(0.8) high women's boots with heels\n",
      "\t(0.8) summer dress for girls with short sleeves and a butterfly print\n",
      "\t(0.8) old iron\n",
      "\t(0.8) a bag of children's clothes\n",
      "\t(0.8) women's jeanses blue and black color\n",
      "\t(0.8) jewelry, braclets\n",
      "\t(0.8) women's leather low boots with buckles view from the top\n",
      "\t(0.8) children's rocking toy in the shape of a dog\n",
      "\t(0.8) dark gray rabbit\n",
      "\t(0.8) ovulation test\n",
      "\t(0.8) women's black and white checkered jacket, burgundy long sleeve shirt, red t-shirt, white printed t-shirt\n",
      "\t(0.8) black kitten sits on the ground\n",
      "\t(0.8) women's white winter boots with fur trim\n",
      "\t(0.8) textbooks for schoolchildren to prepare for exams\n",
      "\t(0.8) reusable diapers\n",
      "\t(1.5) 3 doors wooden cabinet, armchair, wooden chairs\n",
      "\t(1.5) girls' party dress with black appliques\n",
      "\t(1.5) women's pants beige, grey and black\n",
      "\t(1.5) light grey jeans\n",
      "\t(1.5) baby clothes, blue jumpsuit, striped sleepsuits, knitted red warm jumpsuit\n",
      "\t(1.6) red and yellow plastic children's chairs\n",
      "\t(1.6) red and blue boxing gloves\n",
      "\t(1.6) blue jeans\n",
      "\t(1.6) black sleeveless dress for girls\n",
      "\t(1.6) 3d black glasses\n",
      "\t(1.6) tv\n",
      "\t(1.6) red baby blanket on a white chest of drawers\n",
      "\t(1.6) women's high warm black suede boots\n",
      "\t(1.6) warm booties and mittens for a baby\n",
      "\t(2.2) top view of a large black square women's bag with a print of white bows\n",
      "\t(2.4) women's demi-season coat with buttons\n",
      "\t(2.9) white crib bumpers \n",
      "\t(3.1) a bag of children's toys, a bag of children's safety helmets, a children's magnetic construction set\n",
      "\t(3.1) large pregnancy pillow\n",
      "\t(3.1) baby diapers\n",
      "\t(3.1) black kitten sits on the wooden boards\n",
      "\t(3.1) baby's ankle boots\n",
      "\t(3.2) a set of glassware, shot glasses, goblets, a porcelain teapot and sugar bowls, an iron, small flower vase\n",
      "\t(3.2) grey kitten is sitting\n",
      "\t(3.7) sports sneakers\n",
      "\t(3.9) gray knitted women's dress\n",
      "\t(4.5) crib bumpers with bunnies\n",
      "\t(5.4) women's denim dress with long sleeves\n",
      "\t(6.6) plant in a pot\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tp, fp, tn, fn, thr, tp_list, fp_list = calc_metrics(img_markup, model, processor_func)\n",
    "\n",
    "if 2*tp + fp + fn > 0:\n",
    "    f1 = 2*tp/(2*tp + fp + fn)\n",
    "else:\n",
    "    f1 = 0\n",
    "\n",
    "if tp + fp + tn + fn > 0:\n",
    "    acc = (tp + tn)/(tp + fp + tn + fn)\n",
    "else:\n",
    "    acc = 0\n",
    "\n",
    "print(\"TRUE POSITIVES:\\n\\t\" + \"\\n\\t\".join(f\"({dist:.1f}) {txt}\" for dist, txt in tp_list) + \"\\n\\n\")\n",
    "print(\"FALSE POSITIVES:\\n\\t\" + \"\\n\\t\".join(f\"({dist:.1f}) {txt}\" for dist, txt in fp_list) + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5af7a25f-268b-4736-bf43-f12f6cdd173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: unum-cloud/uform-gen2-qwen-500m\n",
      "TP: 78, FP: 139, TN: 76056, FN: 233\n",
      "Threshold: 0.8079373836517334\n",
      "Accuracy: 0.9951376362638225\n",
      "F1: 0.29545454545454547\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "print(f\"Threshold: {thr}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715cdb79-111b-4dce-b62e-5b5649b1133e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
