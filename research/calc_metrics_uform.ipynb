{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639214f6-c836-415c-9062-e33b3f05d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc32849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # works only with old version of transformers\n",
    "# !pip install -U transformers==4.37.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7d2b8f-eb4b-4147-a3fe-c59ac1f93b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # there is a bug in ipykernel that prevents downloading of BAAI/bge-large-en-v1.5, so we need to be sure that it is updated\n",
    "# # https://github.com/huggingface/xet-core/issues/526\n",
    "# !pip install -U ipykernel>=7.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f2e38-d71b-457e-b915-6d283ecc1a27",
   "metadata": {},
   "source": [
    "# Calculate Metrics for UForm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec97dd7e-a891-47b2-934f-10f6e969625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_txt_emb(text):\n",
    "    METRICS_TXT_EMB_MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(METRICS_TXT_EMB_MODEL_NAME)\n",
    "    model = transformers.AutoModel.from_pretrained(METRICS_TXT_EMB_MODEL_NAME)\n",
    "    model.eval()\n",
    "    \n",
    "    encoded_input = tokenizer([text], padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        sentence_embeddings = model_output[0][:, 0]\n",
    "\n",
    "    return torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d2eafe-0f33-4a42-8b84-4e89cb89e0f0",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9e1ee8-259a-4077-bf32-a968e48f3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_markup(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if len(line) == 0:\n",
    "                break\n",
    "            sep_ind = line.find(\"\\\"\")\n",
    "            path = (\"../\" + line[:sep_ind]).strip()\n",
    "            desc = line[sep_ind + 1:].strip()[:-1]\n",
    "            data.append((path, desc))\n",
    "    return data\n",
    "\n",
    "\n",
    "img_markup = load_image_markup(\"../data/matching_images.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956778f6-f7a9-42d9-9f29-499e8d526d6a",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05ad414d-326f-4322-87c3-8ada8c52266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(markup, model, processor_obj, threshold):\n",
    "    PROMPT = \"List the objects in the photo in one sentence, no information about background needed\"\n",
    "\n",
    "    text_list = [markup_line[1].lower() for markup_line in markup]\n",
    "    # some descriptions are the same or mostly same (start from the same words)\n",
    "    unique_indices = []\n",
    "    for idx in range(len(text_list)):\n",
    "        if idx == 0:\n",
    "            unique_indices.append(idx)\n",
    "            continue\n",
    "        prev_started = any(txt.startswith(text_list[idx]) for txt in text_list[:idx])\n",
    "        current_started = any(text_list[idx].startswith(txt) for txt in text_list[:idx])\n",
    "        if prev_started or current_started:\n",
    "            continue\n",
    "        unique_indices.append(idx)\n",
    "    unique_text_list = [text_list[idx] for idx in unique_indices]\n",
    "    emb_unique_text_list = [get_metrics_txt_emb(txt) for txt in unique_text_list]\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for img_idx, markup_line in enumerate(markup):\n",
    "        inputs = processor_obj(text=[PROMPT], images=[PIL.Image.open(markup_line[0])], return_tensors=\"pt\")\n",
    "        with torch.inference_mode():\n",
    "             output = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=False,\n",
    "                use_cache=True,\n",
    "                max_new_tokens=256,\n",
    "                eos_token_id=151645,\n",
    "                pad_token_id=processor_obj.tokenizer.pad_token_id\n",
    "                \n",
    "            )\n",
    "\n",
    "        prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "        decoded_out = processor_obj.batch_decode(output[:, prompt_len:], skip_special_tokens=True)[0]\n",
    "\n",
    "        emb_decoded_out = get_metrics_txt_emb(decoded_out)\n",
    "        distance_per_image = np.array([np.linalg.norm(emb_unique_text - emb_decoded_out) for emb_unique_text in emb_unique_text_list])\n",
    "        # print(f\"dbg: {distance_per_image = }\")  # use this debug printout to setup threshold manualy\n",
    "\n",
    "        if img_idx not in unique_indices:\n",
    "            assert markup_line[1].lower() == text_list[img_idx]\n",
    "            txt_to_find = markup_line[1].lower()\n",
    "            u_img_idx = -1\n",
    "            for i in range(len(unique_text_list)):\n",
    "                if unique_text_list[i].startswith(txt_to_find) or txt_to_find.startswith(unique_text_list[i]):\n",
    "                    u_img_idx = i\n",
    "                    break\n",
    "            assert u_img_idx >= 0\n",
    "            assert u_img_idx < img_idx\n",
    "        else:\n",
    "            u_img_idx = unique_indices.index(img_idx)\n",
    "        found_indices = np.argwhere(distance_per_image <= threshold)[:, 0].tolist()\n",
    "\n",
    "        if u_img_idx in found_indices:\n",
    "            tp += 1\n",
    "            if len(found_indices) > 1:\n",
    "                fp += len(found_indices) - 1\n",
    "            tn += len(unique_indices) - len(found_indices)\n",
    "        else:\n",
    "            fn += 1\n",
    "            fp += len(found_indices)\n",
    "            tn += len(unique_indices) - len(found_indices) - 1\n",
    "\n",
    "    assert tp + fp + tn + fn == len(unique_text_list)*len(text_list)\n",
    "\n",
    "    return tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe4cc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f99fb6e2704aa4882f1bd499beb37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"unum-cloud/uform-gen2-qwen-500m\"\n",
    "model = transformers.AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "processor_func = transformers.AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56614a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIM_THR = 0.88\n",
    "tp, fp, tn, fn = calc_metrics(img_markup, model, processor_func, SIM_THR)\n",
    "\n",
    "if 2*tp + fp + fn > 0:\n",
    "    f1 = 2*tp/(2*tp + fp + fn)\n",
    "else:\n",
    "    f1 = 0\n",
    "\n",
    "if tp + fp + tn + fn > 0:\n",
    "    acc = (tp + tn)/(tp + fp + tn + fn)\n",
    "else:\n",
    "    acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5af7a25f-268b-4736-bf43-f12f6cdd173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: unum-cloud/uform-gen2-qwen-500m\n",
      "TP: 146, FP: 785, TN: 75410, FN: 165\n",
      "Accuracy: 0.9875826732543853\n",
      "F1: 0.23510466988727857\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715cdb79-111b-4dce-b62e-5b5649b1133e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
