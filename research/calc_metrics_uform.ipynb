{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639214f6-c836-415c-9062-e33b3f05d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc32849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # works only with old version of transformers\n",
    "# !pip install -U transformers==4.37.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7d2b8f-eb4b-4147-a3fe-c59ac1f93b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # there is a bug in ipykernel that prevents downloading of BAAI/bge-large-en-v1.5, so we need to be sure that it is updated\n",
    "# # https://github.com/huggingface/xet-core/issues/526\n",
    "# !pip install -U ipykernel>=7.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f2e38-d71b-457e-b915-6d283ecc1a27",
   "metadata": {},
   "source": [
    "# Calculate Metrics for UForm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec97dd7e-a891-47b2-934f-10f6e969625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_txt_emb(text):\n",
    "    METRICS_TXT_EMB_MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(METRICS_TXT_EMB_MODEL_NAME)\n",
    "    model = transformers.AutoModel.from_pretrained(METRICS_TXT_EMB_MODEL_NAME)\n",
    "    model.eval()\n",
    "    \n",
    "    encoded_input = tokenizer([text], padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        sentence_embeddings = model_output[0][:, 0]\n",
    "\n",
    "    return torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d2eafe-0f33-4a42-8b84-4e89cb89e0f0",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9e1ee8-259a-4077-bf32-a968e48f3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_markup(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if len(line) == 0:\n",
    "                break\n",
    "            sep_ind = line.find(\"\\\"\")\n",
    "            path = (\"../\" + line[:sep_ind]).strip()\n",
    "            desc = line[sep_ind + 1:].strip()[:-1]\n",
    "            data.append((path, desc))\n",
    "    return data\n",
    "\n",
    "\n",
    "img_markup = load_image_markup(\"../data/matching_images.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956778f6-f7a9-42d9-9f29-499e8d526d6a",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ad414d-326f-4322-87c3-8ada8c52266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(markup, model, processor_obj):\n",
    "    PROMPT = \"List the objects in the photo in one sentence, no information about background needed\"\n",
    "\n",
    "    text_list = [markup_line[1].lower() for markup_line in markup]\n",
    "    # some descriptions are the same or mostly same (start from the same words)\n",
    "    unique_indices = []\n",
    "    for idx in range(len(text_list)):\n",
    "        if idx == 0:\n",
    "            unique_indices.append(idx)\n",
    "            continue\n",
    "        prev_started = any(txt.startswith(text_list[idx]) for txt in text_list[:idx])\n",
    "        current_started = any(text_list[idx].startswith(txt) for txt in text_list[:idx])\n",
    "        if prev_started or current_started:\n",
    "            continue\n",
    "        unique_indices.append(idx)\n",
    "    unique_text_list = [text_list[idx] for idx in unique_indices]\n",
    "    emb_unique_text_list = [get_metrics_txt_emb(txt) for txt in unique_text_list]\n",
    "\n",
    "    y_true_list = []\n",
    "    distance_list = []\n",
    "    for img_idx, markup_line in enumerate(markup):\n",
    "        inputs = processor_obj(text=[PROMPT], images=[PIL.Image.open(markup_line[0])], return_tensors=\"pt\")\n",
    "        with torch.inference_mode():\n",
    "             output = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=False,\n",
    "                use_cache=True,\n",
    "                max_new_tokens=256,\n",
    "                eos_token_id=151645,\n",
    "                pad_token_id=processor_obj.tokenizer.pad_token_id\n",
    "                \n",
    "            )\n",
    "\n",
    "        prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "        decoded_out = processor_obj.batch_decode(output[:, prompt_len:], skip_special_tokens=True)[0]\n",
    "\n",
    "        emb_decoded_out = get_metrics_txt_emb(decoded_out)\n",
    "        distance_per_image = [np.linalg.norm(emb_unique_text - emb_decoded_out) for emb_unique_text in emb_unique_text_list]\n",
    "\n",
    "        if img_idx not in unique_indices:\n",
    "            assert markup_line[1].lower() == text_list[img_idx]\n",
    "            txt_to_find = markup_line[1].lower()\n",
    "            u_img_idx = -1\n",
    "            for i in range(len(unique_text_list)):\n",
    "                if unique_text_list[i].startswith(txt_to_find) or txt_to_find.startswith(unique_text_list[i]):\n",
    "                    u_img_idx = i\n",
    "                    break\n",
    "            assert u_img_idx >= 0\n",
    "            assert u_img_idx < img_idx\n",
    "        else:\n",
    "            u_img_idx = unique_indices.index(img_idx)\n",
    "\n",
    "        y_true_list += [1 if idx == u_img_idx else 0 for idx in range(len(unique_text_list))]\n",
    "        distance_list += distance_per_image\n",
    "\n",
    "    y_true_vec = np.array(y_true_list)\n",
    "    max_distance = max(distance_list)\n",
    "    logit_vec = np.array([(max_distance - dist)/max_distance for dist in distance_list])\n",
    "    assert y_true_vec.shape == logit_vec.shape\n",
    "\n",
    "    assert logit_vec.min() >= 0\n",
    "    assert logit_vec.max() <= 1\n",
    "    assert np.allclose(np.sort(np.unique(y_true_vec)), np.array([0, 1]))\n",
    "\n",
    "    # https://stats.stackexchange.com/q/287117/\n",
    "    prevalence = np.count_nonzero(y_true_vec == 1, keepdims=False)/len(y_true_vec)\n",
    "    assert prevalence > 0\n",
    "    fpr_vec, tpr_vec, thr_vec = sklearn.metrics.roc_curve(y_true_vec, logit_vec)\n",
    "    recall_vec = tpr_vec\n",
    "    tnr_vec = 1 - fpr_vec\n",
    "\n",
    "    zero_div_idxs = np.where((recall_vec*prevalence) + ((1 - tnr_vec)*(1 - prevalence)) == 0)[0]\n",
    "    if zero_div_idxs.size > 0:\n",
    "        # to avoid zero-division warning from numpy below\n",
    "        recall_vec[zero_div_idxs] += 1e-8\n",
    "    precision_vec = (recall_vec*prevalence)/((recall_vec*prevalence) + ((1 - tnr_vec)*(1 - prevalence)))\n",
    "\n",
    "    zero_div_idxs = np.where(precision_vec + recall_vec == 0)[0]\n",
    "    if zero_div_idxs.size > 0:\n",
    "        # to avoid zero-division warning from numpy below\n",
    "        precision_vec[zero_div_idxs] += 1e-8\n",
    "        recall_vec[zero_div_idxs] += 1e-8\n",
    "    f1_vec = 2*(precision_vec*recall_vec)/(precision_vec + recall_vec)\n",
    "\n",
    "    opt_thr = thr_vec[np.argmax(f1_vec)]\n",
    "    if np.isinf(opt_thr):\n",
    "        opt_thr = 1\n",
    "\n",
    "    # using \">=\" below instead of \">\" is extremely important, because sklearn cn return edge values for threshold\n",
    "    tp = np.count_nonzero((logit_vec >= opt_thr) & (y_true_vec == 1))\n",
    "    fp = np.count_nonzero((logit_vec >= opt_thr) & (y_true_vec == 0))\n",
    "    tn = np.count_nonzero((logit_vec < opt_thr) & (y_true_vec == 0))\n",
    "    fn = np.count_nonzero((logit_vec < opt_thr) & (y_true_vec == 1))\n",
    "\n",
    "    opt_thr = max_distance - (float(opt_thr)*max_distance)\n",
    "\n",
    "    return tp, fp, tn, fn, opt_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fe4cc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02002d39b7bb4aed841c0b7d3d41e06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"unum-cloud/uform-gen2-qwen-500m\"\n",
    "model = transformers.AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "processor_func = transformers.AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56614a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn, thr = calc_metrics(img_markup, model, processor_func)\n",
    "\n",
    "if 2*tp + fp + fn > 0:\n",
    "    f1 = 2*tp/(2*tp + fp + fn)\n",
    "else:\n",
    "    f1 = 0\n",
    "\n",
    "if tp + fp + tn + fn > 0:\n",
    "    acc = (tp + tn)/(tp + fp + tn + fn)\n",
    "else:\n",
    "    acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7a25f-268b-4736-bf43-f12f6cdd173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "print(f\"Threshold: {thr}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715cdb79-111b-4dce-b62e-5b5649b1133e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
